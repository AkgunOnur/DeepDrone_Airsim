{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import system\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals.joblib import dump, load\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], 'datagen/img_generator/'))\n",
    "from network import Net, Net_Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of data:  (20192, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_init_x</th>\n",
       "      <th>true_init_y</th>\n",
       "      <th>true_init_z</th>\n",
       "      <th>blur_coeff</th>\n",
       "      <th>var_sum</th>\n",
       "      <th>diff_x</th>\n",
       "      <th>diff_y</th>\n",
       "      <th>diff_z</th>\n",
       "      <th>diff_phi</th>\n",
       "      <th>diff_theta</th>\n",
       "      <th>diff_psi</th>\n",
       "      <th>r_std</th>\n",
       "      <th>phi_std</th>\n",
       "      <th>theta_std</th>\n",
       "      <th>psi_std</th>\n",
       "      <th>Tf</th>\n",
       "      <th>MP_Method</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.193083</td>\n",
       "      <td>-0.085577</td>\n",
       "      <td>-2.073306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062777</td>\n",
       "      <td>0.173774</td>\n",
       "      <td>-3.445764</td>\n",
       "      <td>0.050844</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>0.715764</td>\n",
       "      <td>0.028558</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.006967</td>\n",
       "      <td>0.025043</td>\n",
       "      <td>2.333994</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>439.659511</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.168562</td>\n",
       "      <td>-0.561884</td>\n",
       "      <td>-2.072260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019096</td>\n",
       "      <td>0.449073</td>\n",
       "      <td>-3.754241</td>\n",
       "      <td>-0.470847</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>0.065251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.008652</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>2.566871</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>238.153363</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.044489</td>\n",
       "      <td>-1.730302</td>\n",
       "      <td>-2.125663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363184</td>\n",
       "      <td>-0.225235</td>\n",
       "      <td>-1.443765</td>\n",
       "      <td>0.094266</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>0.201928</td>\n",
       "      <td>0.323641</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.027552</td>\n",
       "      <td>1.048021</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>30.032970</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.011622</td>\n",
       "      <td>-2.175070</td>\n",
       "      <td>-2.154150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185138</td>\n",
       "      <td>-0.173911</td>\n",
       "      <td>-1.663455</td>\n",
       "      <td>-0.284998</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>0.500841</td>\n",
       "      <td>0.152674</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.024552</td>\n",
       "      <td>1.198461</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>51.768222</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.072312</td>\n",
       "      <td>-2.607443</td>\n",
       "      <td>-2.184460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.450476</td>\n",
       "      <td>0.128647</td>\n",
       "      <td>-1.745246</td>\n",
       "      <td>-0.002598</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>0.132464</td>\n",
       "      <td>0.419632</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023980</td>\n",
       "      <td>1.233005</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>60.806281</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.096470</td>\n",
       "      <td>-3.064906</td>\n",
       "      <td>-2.216762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.249108</td>\n",
       "      <td>1.072036</td>\n",
       "      <td>-3.634816</td>\n",
       "      <td>0.232052</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>0.035242</td>\n",
       "      <td>0.228495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.019430</td>\n",
       "      <td>2.558131</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>227.679519</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.194677</td>\n",
       "      <td>-4.211246</td>\n",
       "      <td>-2.216990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023317</td>\n",
       "      <td>-0.032028</td>\n",
       "      <td>-0.983332</td>\n",
       "      <td>0.429236</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>0.105037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.018312</td>\n",
       "      <td>0.794966</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>16.964613</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.291833</td>\n",
       "      <td>-4.537059</td>\n",
       "      <td>-2.192402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.019956</td>\n",
       "      <td>-0.993838</td>\n",
       "      <td>0.062526</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>0.047952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005209</td>\n",
       "      <td>0.008161</td>\n",
       "      <td>0.007001</td>\n",
       "      <td>0.744850</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>24.577205</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.311979</td>\n",
       "      <td>-4.783995</td>\n",
       "      <td>-2.160646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130170</td>\n",
       "      <td>-0.084228</td>\n",
       "      <td>-0.818867</td>\n",
       "      <td>0.237791</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>0.779829</td>\n",
       "      <td>0.078786</td>\n",
       "      <td>0.010415</td>\n",
       "      <td>0.013663</td>\n",
       "      <td>0.027306</td>\n",
       "      <td>0.654753</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>16.368391</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.306588</td>\n",
       "      <td>-4.987317</td>\n",
       "      <td>-2.131710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.553038</td>\n",
       "      <td>1.376332</td>\n",
       "      <td>-2.729421</td>\n",
       "      <td>-0.228199</td>\n",
       "      <td>-0.021509</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>0.246488</td>\n",
       "      <td>1.471256</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071503</td>\n",
       "      <td>2.084594</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>201.222737</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_init_x  true_init_y  true_init_z  blur_coeff   var_sum    diff_x  \\\n",
       "0    -0.193083    -0.085577    -2.073306         0.0  0.062777  0.173774   \n",
       "1    -0.168562    -0.561884    -2.072260         0.0  0.019096  0.449073   \n",
       "2    -0.044489    -1.730302    -2.125663         0.0  0.363184 -0.225235   \n",
       "3    -0.011622    -2.175070    -2.154150         0.0  0.185138 -0.173911   \n",
       "4    -0.072312    -2.607443    -2.184460         0.0  0.450476  0.128647   \n",
       "5    -0.096470    -3.064906    -2.216762         0.0  0.249108  1.072036   \n",
       "6     0.194677    -4.211246    -2.216990         0.0  0.023317 -0.032028   \n",
       "7     0.291833    -4.537059    -2.192402         0.0  0.020371  0.019956   \n",
       "8     0.311979    -4.783995    -2.160646         0.0  0.130170 -0.084228   \n",
       "9     0.306588    -4.987317    -2.131710         0.0  1.553038  1.376332   \n",
       "\n",
       "     diff_y    diff_z  diff_phi  diff_theta  diff_psi     r_std   phi_std  \\\n",
       "0 -3.445764  0.050844 -0.021509    0.072953  0.715764  0.028558  0.002208   \n",
       "1 -3.754241 -0.470847 -0.021509    0.072953  0.065251  0.000000  0.000098   \n",
       "2 -1.443765  0.094266 -0.021509    0.072953  0.201928  0.323641  0.009191   \n",
       "3 -1.663455 -0.284998 -0.021509    0.072953  0.500841  0.152674  0.006536   \n",
       "4 -1.745246 -0.002598 -0.021509    0.072953  0.132464  0.419632  0.006863   \n",
       "5 -3.634816  0.232052 -0.021509    0.072953  0.035242  0.228495  0.000000   \n",
       "6 -0.983332  0.429236 -0.021509    0.072953  0.105037  0.000000  0.003570   \n",
       "7 -0.993838  0.062526 -0.021509    0.072953  0.047952  0.000000  0.005209   \n",
       "8 -0.818867  0.237791 -0.021509    0.072953  0.779829  0.078786  0.010415   \n",
       "9 -2.729421 -0.228199 -0.021509    0.072953  0.246488  1.471256  0.010279   \n",
       "\n",
       "   theta_std   psi_std        Tf MP_Method        Cost   Status  \n",
       "0   0.006967  0.025043  2.333994   min_vel  439.659511  SUCCESS  \n",
       "1   0.008652  0.010345  2.566871   min_vel  238.153363  SUCCESS  \n",
       "2   0.002800  0.027552  1.048021   min_vel   30.032970  SUCCESS  \n",
       "3   0.001376  0.024552  1.198461   min_vel   51.768222  SUCCESS  \n",
       "4   0.000000  0.023980  1.233005   min_vel   60.806281  SUCCESS  \n",
       "5   0.001183  0.019430  2.558131   min_vel  227.679519  SUCCESS  \n",
       "6   0.001434  0.018312  0.794966   min_vel   16.964613  SUCCESS  \n",
       "7   0.008161  0.007001  0.744850   min_vel   24.577205  SUCCESS  \n",
       "8   0.013663  0.027306  0.654753   min_vel   16.368391  SUCCESS  \n",
       "9   0.000000  0.071503  2.084594   min_vel  201.222737  SUCCESS  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('classifier_files/data.csv')\n",
    "print \"Total length of data: \", data.shape\n",
    "data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution (Before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull flights: 97.6 % \n",
      "Crashed flights: 0.466 % \n",
      "Out of path flights: 0.629 % \n",
      "Collided flights: 1.34 % \n",
      "\n",
      "MP Method distribution, before any restricions\n",
      "Rate of min_vel method in Flights: 27.8 % \n",
      "Rate of min_acc method in Flights: 25.0 % \n",
      "Rate of min_jerk method in Flights: 20.9 % \n",
      "Rate of min_jerk_full_stop method in Flights: 23.9 % \n",
      "Rate of safe_mode in Flights 2.43\n"
     ]
    }
   ],
   "source": [
    "print \"Successfull flights: {0:.3} % \".format(len(data[data[\"Status\"] == \"SUCCESS\"]) / float(len(data)) * 100)\n",
    "print \"Crashed flights: {0:.3} % \".format(len(data[data[\"Status\"] == \"CRASH\"]) / float(len(data)) * 100)\n",
    "print \"Out of path flights: {0:.3} % \".format(len(data[data[\"Status\"] == \"OFF_ROAD\"]) / float(len(data)) * 100)\n",
    "print \"Collided flights: {0:.3} % \".format(len(data[data[\"Status\"] == \"COLLISION\"]) / float(len(data)) * 100)\n",
    "\n",
    "print \"\\nMP Method distribution, before any restricions\"\n",
    "print \"Rate of min_vel method in Flights: {0:.3} % \".format(len(data[(data[\"MP_Method\"] == \"min_vel\") & (data[\"Status\"] == \"SUCCESS\")]) / float(len(data)) * 100)\n",
    "print \"Rate of min_acc method in Flights: {0:.3} % \".format(len(data[(data[\"MP_Method\"] == \"min_acc\") & (data[\"Status\"] == \"SUCCESS\")]) / float(len(data)) * 100)\n",
    "print \"Rate of min_jerk method in Flights: {0:.3} % \".format(len(data[(data[\"MP_Method\"] == \"min_jerk\") & (data[\"Status\"] == \"SUCCESS\")]) / float(len(data)) * 100)\n",
    "print \"Rate of min_jerk_full_stop method in Flights: {0:.3} % \".format(len(data[(data[\"MP_Method\"] == \"min_jerk_full_stop\") & (data[\"Status\"] == \"SUCCESS\")]) / float(len(data)) * 100)\n",
    "print \"Rate of safe_mode in Flights {0:.3}\".format(len(data[data[\"Status\"] != \"SUCCESS\"]) / float(len(data)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution (After)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of flight points, whose cost is below 50.0*median: 96.8 %\n",
      "\n",
      "Data distribution, after median value restriction\n",
      "Rate of min_vel method in Flights: 28.9 % \n",
      "Rate of min_acc method in Flights: 25.8 % \n",
      "Rate of min_jerk method in Flights: 21.1 % \n",
      "Rate of min_jerk_full_stop method in Flights: 24.3 % \n",
      "Upper cost limit: 7716.34\n"
     ]
    }
   ],
   "source": [
    "data = data.sort_values('Cost')\n",
    "median_cost = np.median(data[\"Cost\"])\n",
    "mean_cost = np.mean(data[\"Cost\"])\n",
    "\n",
    "data_coeff = 50.\n",
    "median_data = data[data[\"Cost\"] <= data_coeff*median_cost]\n",
    "unsucessful_flights = data[data[\"Status\"] != \"SUCCESS\"]\n",
    "\n",
    "print \"Percentage of flight points, whose cost is below {0:.3}*median: {1:.3} %\".format(data_coeff, 100. * len(median_data) / len(data))\n",
    "print \"\\nData distribution, after median value restriction\"\n",
    "print \"Rate of min_vel method in Flights: {0:.3} % \".format(len(median_data[median_data[\"MP_Method\"] == \"min_vel\"]) / float(len(median_data)) * 100)\n",
    "print \"Rate of min_acc method in Flights: {0:.3} % \".format(len(median_data[median_data[\"MP_Method\"] == \"min_acc\"]) / float(len(median_data)) * 100)\n",
    "print \"Rate of min_jerk method in Flights: {0:.3} % \".format(len(median_data[median_data[\"MP_Method\"] == \"min_jerk\"]) / float(len(median_data)) * 100)\n",
    "print \"Rate of min_jerk_full_stop method in Flights: {0:.3} % \".format(len(median_data[median_data[\"MP_Method\"] == \"min_jerk_full_stop\"]) / float(len(median_data)) * 100)\n",
    "print \"Upper cost limit: {0:.6}\".format(data_coeff*median_cost)\n",
    "\n",
    "median_arr = median_data.values\n",
    "unsuccessful_arr = unsucessful_flights.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dataset variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_dict = {\"min_vel\":0, \"min_acc\":1, \"min_jerk\":2, \"min_jerk_full_stop\":3}\n",
    "algorithms = [\"min_vel\", \"min_acc\", \"min_jerk\", \"min_jerk_full_stop\"]\n",
    "\n",
    "successful_data = np.copy(median_data.values)\n",
    "unsuccessful_data = np.copy(unsucessful_flights.values)\n",
    "for i, data in enumerate(median_data.values):\n",
    "    successful_data[i][16] = algorithm_dict[data[16]]\n",
    "    \n",
    "for i, data in enumerate(unsucessful_flights.values):\n",
    "    unsuccessful_data[i][16] = 4\n",
    "    \n",
    "successful_classif = np.c_[successful_data[:,4:16], successful_data[:,16]]\n",
    "unsuccessful_classif = np.c_[unsuccessful_data[:,4:16], unsuccessful_data[:,16]]\n",
    "successful_regressor = np.c_[successful_data[:,4:15], successful_data[:,15]]\n",
    "unsuccessful_regressor = np.c_[unsuccessful_data[:,4:15], unsuccessful_data[:,15]]\n",
    "\n",
    "classification_dataset = np.r_[successful_classif, unsuccessful_classif]\n",
    "regression_dataset = np.r_[successful_regressor, unsuccessful_regressor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP Dataset size:  (20033, 13)\n",
      "Regressor Dataset size:  (20033, 12)\n",
      "N_min_vel: 28.16\n",
      "N_min_acc: 25.15\n",
      "N_min_jerk: 20.56\n",
      "N_min_jerk_stop: 23.68\n",
      "N_safe_mode: 2.451\n"
     ]
    }
   ],
   "source": [
    "class_labels = {\"min_vel\":0, \"min_acc\":1, \"min_jerk\":2, \"min_jerk_full_stop\":3, \"safe_mode\":4}\n",
    "N_length = float(classification_dataset.shape[0])\n",
    "print \"MP Dataset size: \",classification_dataset.shape\n",
    "print \"Regressor Dataset size: \",regression_dataset.shape \n",
    "print \"N_min_vel: {0:.4}\".format(np.sum(classification_dataset[:,-1] == class_labels[\"min_vel\"]) / N_length * 100)\n",
    "print \"N_min_acc: {0:.4}\".format(np.sum(classification_dataset[:,-1] == class_labels[\"min_acc\"]) / N_length * 100)\n",
    "print \"N_min_jerk: {0:.4}\".format(np.sum(classification_dataset[:,-1] == class_labels[\"min_jerk\"]) / N_length * 100)\n",
    "print \"N_min_jerk_stop: {0:.4}\".format(np.sum(classification_dataset[:,-1] == class_labels[\"min_jerk_full_stop\"]) / N_length * 100)\n",
    "print \"N_safe_mode: {0:.4}\".format(np.sum(classification_dataset[:,-1] == class_labels[\"safe_mode\"]) / N_length * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split of dataset as train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1_train: (15226, 12) X2_train: (15226, 11)\n",
      "X1_val: (3807, 12) X2_val: (3807, 11)\n",
      "X1_test: (1000, 12) X2_test: (1000, 11)\n"
     ]
    }
   ],
   "source": [
    "N_Test = 1000\n",
    "np.random.shuffle(classification_dataset) # Train 1\n",
    "np.random.shuffle(regression_dataset) # Train 2\n",
    "\n",
    "y1 = np.array(classification_dataset[:,-1],dtype=np.int)\n",
    "y2 = np.array(regression_dataset[:,-1],dtype=np.float)\n",
    "X1 = classification_dataset[:,:-1]\n",
    "X2 = regression_dataset[:,:-1]\n",
    "\n",
    "X1_test = X1[0:N_Test,:]\n",
    "y1_test = y1[0:N_Test]\n",
    "X2_test = X2[0:N_Test,:]\n",
    "y2_test = y2[0:N_Test]\n",
    "\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1[N_Test:,:], y1[N_Test:], test_size=0.2, random_state=42)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2[N_Test:,:], y2[N_Test:], test_size=0.2, random_state=42)\n",
    "    \n",
    "pickle.dump([X1_train, X1_val, X1_test, y1_train, y1_val, y1_test], open(\"classifier_files/dataset1.pkl\",\"wb\"), protocol=2)\n",
    "pickle.dump([X2_train, X2_val, X2_test, y2_train, y2_val, y2_test], open(\"classifier_files/dataset2.pkl\",\"wb\"), protocol=2)\n",
    "\n",
    "scaler1,scaler2 = StandardScaler(), StandardScaler()\n",
    "scaler1.fit(X1_train)\n",
    "scaler2.fit(X2_train)\n",
    "X1_train, X2_train = scaler1.transform(X1_train), scaler2.transform(X2_train)\n",
    "X1_val, X2_val = scaler1.transform(X1_val), scaler2.transform(X2_val)\n",
    "X1_test, X2_test = scaler1.transform(X1_test), scaler2.transform(X2_test)\n",
    "\n",
    "dump(scaler1, 'classifier_files/mp_scaler.bin', compress=True)\n",
    "dump(scaler2, 'classifier_files/time_scaler.bin', compress=True)\n",
    "\n",
    "print \"X1_train: \" + str(X1_train.shape) + \" X2_train: \" + str(X2_train.shape)\n",
    "print \"X1_val: \" + str(X1_val.shape) + \" X2_val: \" + str(X2_val.shape) \n",
    "print \"X1_test: \" + str(X1_test.shape) + \" X2_test: \" + str(X2_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, model, criterion, isClassifier):\n",
    "    #Validation part\n",
    "    model.eval()  # Set model to training mode\n",
    "    \n",
    "    inputs = torch.from_numpy(X).to(device)\n",
    "    if isClassifier:\n",
    "        labels = torch.from_numpy(y).to(device).long()\n",
    "    else:\n",
    "        labels = torch.from_numpy(y).to(device).float()\n",
    "\n",
    "    outputs = model(inputs.float())\n",
    "    if isClassifier:\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    if isClassifier:\n",
    "        accuracy = torch.sum(preds == labels.data).item() / float(inputs.size(0))\n",
    "        print \"Test data, Loss: {0:.3}, Accuracy: {1:.4}\".format(loss.item(), accuracy)\n",
    "    else:\n",
    "        print \"Test data, Loss: {0:.3}\\n\".format(loss.item())\n",
    "        print \"Actual - Predicted\"\n",
    "        pred = outputs.detach().cpu().numpy()\n",
    "        for i in range(y.shape[0]):\n",
    "            print \"{0:.3} - {1:.3}\".format(y[i], outputs[i].item())\n",
    "        \n",
    "    \n",
    "\n",
    "def shuffle_dataset(X, y):\n",
    "    p = np.random.permutation(len(X))\n",
    "    return X[p], y[p]\n",
    "\n",
    "def train_model(isClassifier, X, y, X_val, y_val, model, criterion, optimizer, scheduler, minibatch_size, name, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_train_acc = 0.0\n",
    "    best_acc = 0.0\n",
    "    best_loss = 1e6\n",
    "    best_train_loss = 1e6\n",
    "    \n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracy_train = []\n",
    "    accuracy_val = []\n",
    "    # path = F\"/content/drive/My Drive/best_model.pt\"\n",
    "#     directory = path_name\n",
    "\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        X_train, y_train = shuffle_dataset(X, y)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        losses_iter = []\n",
    "        accuracy_iter = []\n",
    "\n",
    "        # Iterate over data.\n",
    "        for i in range(0, X_train.shape[0], minibatch_size):\n",
    "            # Get pair of (X, y) of the current minibatch/chunk             \n",
    "            X_batch = X_train[i:i + minibatch_size]\n",
    "            y_batch = y_train[i:i + minibatch_size]\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            xbatch = torch.from_numpy(X_batch).to(device)\n",
    "            \n",
    "            if isClassifier:\n",
    "                ybatch = torch.from_numpy(y_batch).to(device).long()\n",
    "            else:\n",
    "                ybatch = torch.from_numpy(y_batch).to(device).float()\n",
    "\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                pred = model(xbatch.float())\n",
    "                if isClassifier:\n",
    "                    _, preds = torch.max(pred, 1)\n",
    "                \n",
    "                loss = criterion(pred, ybatch)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            # statistics\n",
    "            running_loss += loss.item() * xbatch.size(0)\n",
    "            if isClassifier:\n",
    "                running_corrects += torch.sum(preds == ybatch.data)\n",
    "\n",
    "            # print (\"losses_iter\", loss.item() * inputs.size(0))\n",
    "            # print (\"accuracy_iter\", torch.sum(preds == labels.data).item() / float(inputs.size(0)))\n",
    "\n",
    "            losses_iter.append(loss.item())\n",
    "            if isClassifier:\n",
    "                accuracy_iter.append(torch.sum(preds == ybatch.data).item() / float(xbatch.size(0)))\n",
    "        \n",
    "        \n",
    "        train_loss = np.mean(losses_iter)\n",
    "        losses_train.append(train_loss)\n",
    "        if isClassifier:\n",
    "            train_acc = np.mean(accuracy_iter)\n",
    "            accuracy_train.append(train_acc)\n",
    "            print'Training Loss: {:.4f} Acc: {:.4f}'.format(train_loss, train_acc)  \n",
    "        else:\n",
    "            print'Training Loss: {:.4f}'.format(train_loss)\n",
    " \n",
    "        \n",
    "        \n",
    "        #Validation part\n",
    "        model.eval()  # Set model to training mode\n",
    "        \n",
    "        xbatch = torch.from_numpy(X_val).to(device)\n",
    "        if isClassifier:\n",
    "            ybatch = torch.from_numpy(y_val).to(device).long()\n",
    "        else:\n",
    "            ybatch = torch.from_numpy(y_val).to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(xbatch.float())\n",
    "        if isClassifier:\n",
    "            _, preds = torch.max(pred, 1)\n",
    "\n",
    "        loss = criterion(pred, ybatch)\n",
    "        \n",
    "        val_loss = loss.item()\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if isClassifier:\n",
    "            val_acc = torch.sum(preds == ybatch.data).item() / float(xbatch.size(0))\n",
    "            accuracy_val.append(val_acc)\n",
    "        \n",
    "        losses_val.append(val_loss)\n",
    "        \n",
    "        \n",
    "        if isClassifier:\n",
    "            print'Validation Loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc)\n",
    "        else:\n",
    "            print'Validation Loss: {:.4f}'.format(val_loss)\n",
    "\n",
    "#         deep copy the model\n",
    "        if isClassifier:\n",
    "            if val_acc > best_acc:\n",
    "                best_train_acc = train_acc\n",
    "                best_acc = val_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                path = name + \"_best_model.pt\"\n",
    "                torch.save(best_model_wts, \"classifier_files/\" + path)\n",
    "        else:\n",
    "            if val_loss < best_loss:\n",
    "                best_train_loss = train_loss\n",
    "                best_loss = val_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                path = name + \"_best_model.pt\"\n",
    "                torch.save(best_model_wts, \"classifier_files/\" + path)\n",
    "\n",
    "        print \"\"\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print'Training complete in {:.0f}s'.format(time_elapsed)\n",
    "    if isClassifier:\n",
    "        print'Best Train Acc: {:4f}'.format(best_train_acc)\n",
    "        print'Best Val Acc: {:4f}'.format(best_acc)\n",
    "    else:\n",
    "        print'Best Train Loss: {:4f}'.format(best_train_loss)\n",
    "        print'Best Val Loss: {:4f}'.format(best_loss)\n",
    "\n",
    "    \n",
    "#     stats_columns = ['Layers', 'Epochs', 'BatchSize', 'LearningRate', 'Optimizer', 'Scheduler', 'TrainAcc', 'ValAcc']\n",
    "#     layers = [module for module in model.modules() if type(module) != nn.Sequential]\n",
    "    #write_results([layers, num_epochs, minibatch_size, learning_rate, optimizer.state_dict, scheduler.state_dict, best_train_acc, best_val_acc])\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MP Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "Training Loss: 1.4476 Acc: 0.2965\n",
      "Validation Loss: 1.4235 Acc: 0.3081\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "Training Loss: 1.4177 Acc: 0.3165\n",
      "Validation Loss: 1.4118 Acc: 0.3210\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "Training Loss: 1.4037 Acc: 0.3306\n",
      "Validation Loss: 1.4070 Acc: 0.3218\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "Training Loss: 1.3935 Acc: 0.3349\n",
      "Validation Loss: 1.3978 Acc: 0.3362\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "Training Loss: 1.3833 Acc: 0.3432\n",
      "Validation Loss: 1.3997 Acc: 0.3396\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "Training Loss: 1.3745 Acc: 0.3521\n",
      "Validation Loss: 1.3918 Acc: 0.3404\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "Training Loss: 1.3613 Acc: 0.3602\n",
      "Validation Loss: 1.3921 Acc: 0.3410\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "Training Loss: 1.3526 Acc: 0.3733\n",
      "Validation Loss: 1.3894 Acc: 0.3496\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "Training Loss: 1.3446 Acc: 0.3770\n",
      "Validation Loss: 1.3739 Acc: 0.3664\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "Training Loss: 1.3330 Acc: 0.3821\n",
      "Validation Loss: 1.3730 Acc: 0.3604\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "Training Loss: 1.3250 Acc: 0.3924\n",
      "Validation Loss: 1.3653 Acc: 0.3701\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "Training Loss: 1.3192 Acc: 0.3928\n",
      "Validation Loss: 1.3633 Acc: 0.3685\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "Training Loss: 1.3064 Acc: 0.4028\n",
      "Validation Loss: 1.3576 Acc: 0.3830\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "Training Loss: 1.2967 Acc: 0.4131\n",
      "Validation Loss: 1.3627 Acc: 0.3780\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "Training Loss: 1.2860 Acc: 0.4168\n",
      "Validation Loss: 1.3614 Acc: 0.3919\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "Training Loss: 1.2783 Acc: 0.4198\n",
      "Validation Loss: 1.3434 Acc: 0.3867\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "Training Loss: 1.2711 Acc: 0.4274\n",
      "Validation Loss: 1.3482 Acc: 0.3906\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "Training Loss: 1.2632 Acc: 0.4383\n",
      "Validation Loss: 1.3501 Acc: 0.3964\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "Training Loss: 1.2549 Acc: 0.4414\n",
      "Validation Loss: 1.3308 Acc: 0.4058\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "Training Loss: 1.2451 Acc: 0.4453\n",
      "Validation Loss: 1.3489 Acc: 0.4027\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "Training Loss: 1.2397 Acc: 0.4497\n",
      "Validation Loss: 1.3388 Acc: 0.4043\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "Training Loss: 1.2300 Acc: 0.4610\n",
      "Validation Loss: 1.3293 Acc: 0.4008\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "Training Loss: 1.2245 Acc: 0.4570\n",
      "Validation Loss: 1.3373 Acc: 0.4001\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "Training Loss: 1.2174 Acc: 0.4656\n",
      "Validation Loss: 1.3292 Acc: 0.4024\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "Training Loss: 1.2119 Acc: 0.4718\n",
      "Validation Loss: 1.3363 Acc: 0.4116\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "Training Loss: 1.2014 Acc: 0.4760\n",
      "Validation Loss: 1.3282 Acc: 0.4108\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "Training Loss: 1.1990 Acc: 0.4716\n",
      "Validation Loss: 1.3263 Acc: 0.4056\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "Training Loss: 1.1849 Acc: 0.4819\n",
      "Validation Loss: 1.3348 Acc: 0.4100\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "Training Loss: 1.1812 Acc: 0.4828\n",
      "Validation Loss: 1.3287 Acc: 0.4158\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "Training Loss: 1.1768 Acc: 0.4878\n",
      "Validation Loss: 1.3327 Acc: 0.4205\n",
      "\n",
      "Training complete in 55s\n",
      "Best Train Acc: 0.487815\n",
      "Best Val Acc: 0.420541\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop_layer = nn.Dropout(p=0.1)\n",
    "        self.fc1 = nn.Linear(12, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "n_epochs = 30\n",
    "minibatch_size = 16\n",
    "learning_rate = 0.001\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "ce_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, X_train.shape[0], eta_min=learning_rate)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "mp_model = train_model(True, X1_train, y1_train, X1_val, y1_val, model, ce_criterion, optimizer, scheduler, minibatch_size, \"mp_classifier\", num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for MP Classification\n",
      "Test data, Loss: 1.3, Accuracy: 0.438\n"
     ]
    }
   ],
   "source": [
    "print \"Test Results for MP Classification\"\n",
    "predict(X1_test, y1_test, mp_model, ce_criterion, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Regressor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "Training Loss: 1.9939\n",
      "Validation Loss: 1.8672\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "Training Loss: 1.8374\n",
      "Validation Loss: 1.8779\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "Training Loss: 1.8271\n",
      "Validation Loss: 1.8778\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "Training Loss: 1.8154\n",
      "Validation Loss: 1.8447\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "Training Loss: 1.8089\n",
      "Validation Loss: 1.8614\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "Training Loss: 1.8051\n",
      "Validation Loss: 1.8508\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "Training Loss: 1.7996\n",
      "Validation Loss: 1.8460\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "Training Loss: 1.7981\n",
      "Validation Loss: 1.8576\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "Training Loss: 1.7949\n",
      "Validation Loss: 1.8463\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "Training Loss: 1.7938\n",
      "Validation Loss: 1.8482\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "Training Loss: 1.7881\n",
      "Validation Loss: 1.8472\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "Training Loss: 1.7886\n",
      "Validation Loss: 1.8495\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "Training Loss: 1.7868\n",
      "Validation Loss: 1.8469\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "Training Loss: 1.7889\n",
      "Validation Loss: 1.8425\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "Training Loss: 1.7842\n",
      "Validation Loss: 1.8446\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "Training Loss: 1.7811\n",
      "Validation Loss: 1.8423\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "Training Loss: 1.7831\n",
      "Validation Loss: 1.8425\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "Training Loss: 1.7809\n",
      "Validation Loss: 1.8525\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "Training Loss: 1.7822\n",
      "Validation Loss: 1.8434\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "Training Loss: 1.7791\n",
      "Validation Loss: 1.8442\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "Training Loss: 1.7805\n",
      "Validation Loss: 1.8452\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "Training Loss: 1.7793\n",
      "Validation Loss: 1.8418\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "Training Loss: 1.7810\n",
      "Validation Loss: 1.8440\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "Training Loss: 1.7791\n",
      "Validation Loss: 1.8431\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "Training Loss: 1.7796\n",
      "Validation Loss: 1.8448\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "Training Loss: 1.7778\n",
      "Validation Loss: 1.8430\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "Training Loss: 1.7775\n",
      "Validation Loss: 1.8424\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "Training Loss: 1.7761\n",
      "Validation Loss: 1.8435\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "Training Loss: 1.7765\n",
      "Validation Loss: 1.8428\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "Training Loss: 1.7766\n",
      "Validation Loss: 1.8411\n",
      "\n",
      "Training complete in 21s\n",
      "Best Train Loss: 1.776588\n",
      "Best Val Loss: 1.841146\n"
     ]
    }
   ],
   "source": [
    "class Net_Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_Regressor, self).__init__()\n",
    "        self.drop_layer = nn.Dropout(p=0.15)\n",
    "        self.fc1 = nn.Linear(11, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "n_epochs = 30\n",
    "minibatch_size = 16\n",
    "learning_rate = 0.001\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = Net_Regressor()\n",
    "model = model.to(device)\n",
    "mse_criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, X_train.shape[0], eta_min=learning_rate)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "time_model = train_model(False, X2_train, y2_train, X2_val, y2_val, model, mse_criterion, optimizer, scheduler, minibatch_size, 'time_regressor', num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for Time Regression\n",
      "Test data, Loss: 0.651\n",
      "\n",
      "Actual - Predicted\n",
      "1.14 - 1.52\n",
      "1.93 - 1.56\n",
      "2.17 - 1.55\n",
      "0.868 - 1.52\n",
      "1.08 - 1.53\n",
      "1.68 - 1.52\n",
      "1.69 - 1.53\n",
      "1.19 - 1.54\n",
      "0.902 - 1.52\n",
      "1.0 - 1.52\n",
      "2.21 - 1.55\n",
      "0.921 - 1.53\n",
      "1.36 - 1.52\n",
      "0.894 - 1.51\n",
      "1.01 - 1.52\n",
      "1.91 - 1.54\n",
      "1.2 - 1.53\n",
      "1.44 - 1.52\n",
      "1.4 - 1.53\n",
      "1.5 - 1.55\n",
      "1.56 - 1.55\n",
      "1.98 - 1.53\n",
      "1.27 - 1.52\n",
      "1.24 - 1.55\n",
      "1.31 - 1.53\n",
      "1.68 - 1.54\n",
      "2.47 - 1.56\n",
      "1.36 - 1.54\n",
      "1.55 - 1.53\n",
      "1.77 - 1.53\n",
      "1.17 - 1.54\n",
      "2.67 - 1.58\n",
      "1.97 - 1.55\n",
      "1.46 - 1.54\n",
      "0.401 - 1.51\n",
      "2.14 - 1.6\n",
      "1.14 - 1.51\n",
      "1.55 - 1.54\n",
      "0.784 - 1.51\n",
      "1.27 - 1.54\n",
      "1.7 - 1.57\n",
      "1.49 - 1.56\n",
      "1.67 - 1.53\n",
      "1.57 - 1.55\n",
      "1.26 - 1.53\n",
      "1.18 - 1.52\n",
      "0.641 - 1.52\n",
      "1.36 - 1.54\n",
      "0.639 - 1.52\n",
      "1.58 - 1.54\n",
      "0.987 - 1.53\n",
      "2.44 - 1.56\n",
      "2.39 - 1.57\n",
      "1.23 - 1.54\n",
      "0.996 - 1.52\n",
      "0.982 - 1.53\n",
      "2.13 - 1.56\n",
      "1.29 - 1.53\n",
      "1.68 - 1.54\n",
      "1.05 - 1.52\n",
      "2.54 - 1.57\n",
      "1.04 - 1.53\n",
      "1.15 - 1.54\n",
      "2.11 - 1.54\n",
      "1.4 - 1.55\n",
      "1.3 - 1.53\n",
      "0.969 - 1.56\n",
      "0.87 - 1.52\n",
      "2.06 - 1.54\n",
      "1.1 - 1.53\n",
      "2.83 - 1.58\n",
      "1.31 - 1.54\n",
      "1.04 - 1.53\n",
      "0.987 - 1.53\n",
      "1.19 - 1.52\n",
      "1.02 - 1.53\n",
      "0.705 - 1.51\n",
      "1.33 - 1.53\n",
      "1.56 - 1.54\n",
      "1.27 - 1.54\n",
      "1.7 - 1.54\n",
      "2.02 - 1.54\n",
      "2.26 - 1.55\n",
      "2.06 - 1.54\n",
      "1.87 - 1.54\n",
      "1.27 - 1.52\n",
      "1.59 - 1.52\n",
      "1.36 - 1.52\n",
      "1.22 - 1.52\n",
      "1.24 - 1.52\n",
      "1.06 - 1.52\n",
      "2.74 - 1.61\n",
      "1.73 - 1.54\n",
      "1.35 - 1.54\n",
      "1.87 - 1.57\n",
      "1.38 - 1.54\n",
      "1.04 - 1.52\n",
      "1.67 - 1.52\n",
      "1.68 - 1.56\n",
      "1.5 - 1.53\n",
      "1.71 - 1.53\n",
      "1.02 - 1.52\n",
      "1.34 - 1.53\n",
      "1.88 - 1.54\n",
      "1.62 - 1.53\n",
      "7.38 - 1.74\n",
      "2.17 - 1.54\n",
      "0.987 - 1.51\n",
      "0.534 - 1.52\n",
      "1.42 - 1.54\n",
      "1.04 - 1.52\n",
      "1.92 - 1.54\n",
      "0.753 - 1.51\n",
      "0.527 - 1.52\n",
      "0.758 - 1.51\n",
      "1.6 - 1.54\n",
      "2.17 - 1.57\n",
      "1.24 - 1.52\n",
      "2.29 - 1.55\n",
      "1.05 - 1.52\n",
      "1.25 - 1.55\n",
      "2.37 - 1.6\n",
      "0.485 - 1.51\n",
      "0.836 - 1.53\n",
      "1.14 - 1.53\n",
      "0.679 - 1.53\n",
      "1.46 - 1.52\n",
      "0.915 - 1.52\n",
      "2.32 - 1.59\n",
      "0.59 - 1.5\n",
      "1.93 - 1.53\n",
      "2.13 - 1.55\n",
      "2.69 - 1.6\n",
      "2.11 - 1.56\n",
      "2.51 - 1.56\n",
      "1.55 - 1.53\n",
      "1.87 - 1.54\n",
      "2.22 - 1.56\n",
      "1.37 - 1.53\n",
      "1.09 - 1.53\n",
      "1.95 - 1.54\n",
      "2.45 - 1.56\n",
      "1.24 - 1.53\n",
      "1.18 - 1.53\n",
      "1.23 - 1.52\n",
      "0.682 - 1.51\n",
      "1.1 - 1.52\n",
      "1.56 - 1.52\n",
      "2.52 - 1.59\n",
      "2.46 - 1.57\n",
      "1.42 - 1.54\n",
      "1.45 - 1.52\n",
      "0.586 - 1.5\n",
      "2.16 - 1.54\n",
      "1.28 - 1.54\n",
      "0.955 - 1.52\n",
      "1.7 - 1.53\n",
      "2.03 - 1.55\n",
      "0.814 - 1.53\n",
      "2.02 - 1.56\n",
      "1.86 - 1.53\n",
      "1.32 - 1.52\n",
      "1.49 - 1.54\n",
      "0.699 - 1.52\n",
      "1.42 - 1.52\n",
      "2.76 - 1.58\n",
      "1.04 - 1.52\n",
      "2.39 - 1.56\n",
      "1.81 - 1.53\n",
      "1.25 - 1.52\n",
      "1.78 - 1.54\n",
      "2.59 - 1.55\n",
      "1.4 - 1.53\n",
      "1.22 - 1.55\n",
      "1.17 - 1.54\n",
      "1.18 - 1.53\n",
      "1.98 - 1.54\n",
      "1.09 - 1.52\n",
      "1.24 - 1.53\n",
      "0.958 - 1.52\n",
      "1.32 - 1.55\n",
      "0.807 - 1.52\n",
      "1.61 - 1.54\n",
      "1.87 - 1.57\n",
      "1.46 - 1.52\n",
      "1.8 - 1.55\n",
      "1.1 - 1.54\n",
      "1.85 - 1.55\n",
      "0.948 - 1.52\n",
      "7.01 - 1.81\n",
      "1.48 - 1.54\n",
      "1.55 - 1.55\n",
      "1.14 - 1.54\n",
      "0.683 - 1.51\n",
      "0.919 - 1.52\n",
      "1.63 - 1.53\n",
      "1.99 - 1.54\n",
      "1.6 - 1.55\n",
      "1.09 - 1.52\n",
      "1.93 - 1.54\n",
      "1.19 - 1.54\n",
      "1.31 - 1.54\n",
      "0.987 - 1.53\n",
      "1.58 - 1.54\n",
      "1.77 - 1.56\n",
      "1.57 - 1.55\n",
      "1.71 - 1.54\n",
      "0.702 - 1.51\n",
      "1.97 - 1.56\n",
      "1.51 - 1.57\n",
      "0.771 - 1.52\n",
      "1.26 - 1.52\n",
      "0.703 - 1.51\n",
      "1.69 - 1.54\n",
      "0.579 - 1.51\n",
      "1.05 - 1.53\n",
      "1.01 - 1.52\n",
      "0.737 - 1.51\n",
      "1.59 - 1.55\n",
      "1.14 - 1.54\n",
      "0.586 - 1.52\n",
      "0.994 - 1.51\n",
      "1.78 - 1.54\n",
      "1.41 - 1.55\n",
      "1.52 - 1.55\n",
      "2.59 - 1.6\n",
      "1.58 - 1.54\n",
      "1.32 - 1.53\n",
      "1.68 - 1.53\n",
      "0.585 - 1.5\n",
      "2.27 - 1.58\n",
      "1.67 - 1.55\n",
      "1.38 - 1.54\n",
      "2.07 - 1.54\n",
      "1.32 - 1.52\n",
      "1.96 - 1.54\n",
      "1.02 - 1.5\n",
      "1.18 - 1.53\n",
      "1.88 - 1.55\n",
      "1.59 - 1.56\n",
      "1.26 - 1.52\n",
      "1.99 - 1.55\n",
      "0.824 - 1.51\n",
      "0.907 - 1.51\n",
      "1.61 - 1.55\n",
      "2.74 - 1.61\n",
      "5.59 - 1.69\n",
      "1.02 - 1.53\n",
      "1.37 - 1.53\n",
      "1.17 - 1.53\n",
      "1.32 - 1.55\n",
      "0.935 - 1.52\n",
      "1.74 - 1.55\n",
      "2.22 - 1.58\n",
      "1.18 - 1.52\n",
      "1.46 - 1.53\n",
      "1.75 - 1.54\n",
      "0.783 - 1.52\n",
      "0.905 - 1.51\n",
      "1.83 - 1.58\n",
      "1.32 - 1.54\n",
      "1.3 - 1.54\n",
      "1.46 - 1.55\n",
      "2.34 - 1.56\n",
      "0.707 - 1.52\n",
      "1.31 - 1.52\n",
      "1.34 - 1.55\n",
      "2.25 - 1.57\n",
      "1.74 - 1.52\n",
      "1.29 - 1.52\n",
      "1.97 - 1.55\n",
      "1.19 - 1.53\n",
      "1.02 - 1.52\n",
      "1.25 - 1.52\n",
      "1.49 - 1.52\n",
      "1.56 - 1.55\n",
      "2.24 - 1.55\n",
      "1.77 - 1.54\n",
      "1.85 - 1.56\n",
      "0.965 - 1.51\n",
      "1.84 - 1.53\n",
      "1.03 - 1.51\n",
      "1.62 - 1.53\n",
      "1.24 - 1.52\n",
      "0.75 - 1.52\n",
      "1.72 - 1.57\n",
      "1.36 - 1.54\n",
      "1.41 - 1.51\n",
      "0.979 - 1.53\n",
      "1.59 - 1.54\n",
      "1.74 - 1.53\n",
      "1.13 - 1.54\n",
      "1.23 - 1.52\n",
      "1.18 - 1.53\n",
      "1.36 - 1.52\n",
      "1.18 - 1.52\n",
      "1.15 - 1.52\n",
      "1.79 - 1.54\n",
      "1.01 - 1.52\n",
      "1.7 - 1.53\n",
      "1.77 - 1.53\n",
      "1.3 - 1.54\n",
      "0.493 - 1.5\n",
      "2.25 - 1.55\n",
      "1.48 - 1.53\n",
      "1.05 - 1.52\n",
      "1.41 - 1.52\n",
      "1.53 - 1.55\n",
      "2.96 - 1.66\n",
      "1.72 - 1.54\n",
      "0.96 - 1.52\n",
      "2.03 - 1.55\n",
      "1.82 - 1.55\n",
      "1.36 - 1.54\n",
      "0.65 - 1.48\n",
      "1.71 - 1.54\n",
      "1.56 - 1.55\n",
      "1.59 - 1.55\n",
      "1.45 - 1.52\n",
      "1.52 - 1.53\n",
      "2.33 - 1.56\n",
      "1.16 - 1.53\n",
      "1.02 - 1.52\n",
      "1.79 - 1.57\n",
      "1.27 - 1.53\n",
      "1.4 - 1.54\n",
      "0.917 - 1.53\n",
      "2.02 - 1.55\n",
      "1.46 - 1.55\n",
      "1.0 - 1.53\n",
      "1.06 - 1.52\n",
      "1.61 - 1.57\n",
      "2.44 - 1.58\n",
      "1.69 - 1.57\n",
      "0.802 - 1.52\n",
      "1.92 - 1.53\n",
      "0.594 - 1.51\n",
      "1.07 - 1.52\n",
      "1.13 - 1.52\n",
      "0.863 - 1.52\n",
      "1.93 - 1.54\n",
      "1.01 - 1.53\n",
      "1.04 - 1.54\n",
      "1.56 - 1.54\n",
      "2.05 - 1.58\n",
      "2.15 - 1.57\n",
      "0.64 - 1.52\n",
      "1.1 - 1.53\n",
      "1.19 - 1.52\n",
      "1.96 - 1.54\n",
      "1.6 - 1.54\n",
      "2.09 - 1.55\n",
      "2.27 - 1.56\n",
      "0.754 - 1.52\n",
      "1.4 - 1.55\n",
      "1.39 - 1.54\n",
      "1.78 - 1.56\n",
      "1.42 - 1.53\n",
      "2.4 - 1.58\n",
      "0.799 - 1.51\n",
      "1.59 - 1.55\n",
      "1.14 - 1.54\n",
      "2.26 - 1.54\n",
      "1.84 - 1.56\n",
      "2.18 - 1.56\n",
      "1.36 - 1.54\n",
      "7.83 - 1.76\n",
      "0.929 - 1.51\n",
      "1.77 - 1.54\n",
      "1.96 - 1.57\n",
      "1.94 - 1.55\n",
      "1.64 - 1.54\n",
      "1.22 - 1.52\n",
      "1.05 - 1.52\n",
      "0.921 - 1.5\n",
      "1.83 - 1.56\n",
      "1.72 - 1.55\n",
      "0.864 - 1.51\n",
      "1.49 - 1.53\n",
      "1.31 - 1.52\n",
      "0.489 - 1.51\n",
      "1.51 - 1.55\n",
      "0.552 - 1.5\n",
      "1.23 - 1.53\n",
      "1.67 - 1.55\n",
      "1.47 - 1.54\n",
      "2.04 - 1.52\n",
      "1.79 - 1.54\n",
      "1.49 - 1.53\n",
      "1.36 - 1.53\n",
      "1.46 - 1.52\n",
      "0.799 - 1.51\n",
      "1.76 - 1.54\n",
      "0.858 - 1.52\n",
      "6.43 - 1.8\n",
      "3.17 - 1.57\n",
      "1.84 - 1.57\n",
      "1.31 - 1.53\n",
      "1.68 - 1.53\n",
      "1.62 - 1.56\n",
      "1.36 - 1.53\n",
      "1.36 - 1.54\n",
      "1.25 - 1.54\n",
      "1.83 - 1.57\n",
      "0.995 - 1.53\n",
      "2.04 - 1.58\n",
      "0.844 - 1.51\n",
      "1.95 - 1.55\n",
      "2.31 - 1.6\n",
      "1.53 - 1.53\n",
      "2.02 - 1.55\n",
      "1.31 - 1.54\n",
      "0.809 - 1.51\n",
      "0.766 - 1.53\n",
      "1.39 - 1.53\n",
      "1.47 - 1.53\n",
      "0.555 - 1.52\n",
      "0.619 - 1.51\n",
      "0.78 - 1.51\n",
      "1.06 - 1.53\n",
      "2.97 - 1.6\n",
      "0.869 - 1.52\n",
      "1.58 - 1.52\n",
      "1.45 - 1.53\n",
      "1.15 - 1.54\n",
      "1.5 - 1.54\n",
      "1.99 - 1.54\n",
      "1.65 - 1.55\n",
      "0.393 - 1.51\n",
      "0.858 - 1.51\n",
      "1.43 - 1.53\n",
      "0.544 - 1.5\n",
      "1.21 - 1.52\n",
      "1.15 - 1.53\n",
      "1.77 - 1.54\n",
      "1.6 - 1.55\n",
      "1.63 - 1.52\n",
      "0.859 - 1.51\n",
      "1.81 - 1.55\n",
      "1.86 - 1.54\n",
      "1.71 - 1.55\n",
      "1.2 - 1.53\n",
      "1.81 - 1.53\n",
      "1.7 - 1.53\n",
      "0.688 - 1.51\n",
      "1.18 - 1.53\n",
      "1.94 - 1.58\n",
      "1.87 - 1.55\n",
      "1.74 - 1.56\n",
      "0.663 - 1.51\n",
      "0.44 - 1.51\n",
      "0.968 - 1.53\n",
      "2.64 - 1.63\n",
      "1.91 - 1.55\n",
      "1.65 - 1.55\n",
      "1.17 - 1.53\n",
      "1.35 - 1.52\n",
      "1.6 - 1.53\n",
      "1.8 - 1.54\n",
      "0.924 - 1.51\n",
      "1.8 - 1.55\n",
      "2.7 - 1.57\n",
      "1.19 - 1.53\n",
      "1.11 - 1.52\n",
      "2.32 - 1.58\n",
      "1.24 - 1.51\n",
      "1.56 - 1.54\n",
      "1.21 - 1.53\n",
      "7.23 - 1.74\n",
      "2.3 - 1.57\n",
      "1.55 - 1.53\n",
      "1.33 - 1.53\n",
      "3.91 - 1.73\n",
      "0.856 - 1.51\n",
      "1.91 - 1.54\n",
      "1.09 - 1.54\n",
      "2.22 - 1.57\n",
      "1.63 - 1.52\n",
      "1.31 - 1.54\n",
      "1.14 - 1.54\n",
      "2.37 - 1.6\n",
      "2.18 - 1.56\n",
      "2.23 - 1.56\n",
      "1.83 - 1.56\n",
      "1.16 - 1.54\n",
      "1.93 - 1.56\n",
      "1.85 - 1.54\n",
      "1.87 - 1.56\n",
      "2.22 - 1.54\n",
      "2.02 - 1.58\n",
      "1.5 - 1.52\n",
      "1.13 - 1.53\n",
      "3.66 - 1.59\n",
      "1.86 - 1.55\n",
      "1.39 - 1.53\n",
      "1.23 - 1.52\n",
      "1.67 - 1.53\n",
      "1.4 - 1.55\n",
      "1.4 - 1.55\n",
      "1.32 - 1.54\n",
      "1.48 - 1.53\n",
      "2.19 - 1.55\n",
      "0.34 - 1.51\n",
      "1.72 - 1.53\n",
      "0.853 - 1.51\n",
      "1.78 - 1.54\n",
      "1.34 - 1.53\n",
      "1.35 - 1.54\n",
      "1.29 - 1.52\n",
      "0.9 - 1.53\n",
      "1.42 - 1.54\n",
      "1.4 - 1.52\n",
      "1.48 - 1.52\n",
      "0.321 - 1.48\n",
      "1.17 - 1.52\n",
      "2.19 - 1.55\n",
      "1.15 - 1.53\n",
      "1.99 - 1.55\n",
      "1.32 - 1.54\n",
      "1.39 - 1.52\n",
      "1.72 - 1.53\n",
      "1.39 - 1.54\n",
      "1.2 - 1.54\n",
      "0.815 - 1.52\n",
      "1.43 - 1.53\n",
      "1.78 - 1.54\n",
      "2.8 - 1.62\n",
      "1.92 - 1.54\n",
      "0.931 - 1.52\n",
      "1.08 - 1.54\n",
      "1.9 - 1.54\n",
      "2.19 - 1.55\n",
      "1.59 - 1.55\n",
      "2.08 - 1.56\n",
      "2.12 - 1.54\n",
      "1.49 - 1.53\n",
      "1.27 - 1.53\n",
      "0.728 - 1.52\n",
      "1.38 - 1.53\n",
      "0.977 - 1.52\n",
      "1.56 - 1.54\n",
      "0.958 - 1.52\n",
      "1.11 - 1.54\n",
      "1.64 - 1.55\n",
      "2.29 - 1.57\n",
      "1.47 - 1.54\n",
      "0.781 - 1.52\n",
      "1.73 - 1.53\n",
      "1.96 - 1.56\n",
      "3.92 - 1.59\n",
      "1.75 - 1.53\n",
      "1.59 - 1.57\n",
      "1.23 - 1.52\n",
      "1.89 - 1.55\n",
      "0.994 - 1.53\n",
      "2.02 - 1.56\n",
      "1.29 - 1.53\n",
      "1.08 - 1.54\n",
      "1.53 - 1.53\n",
      "1.57 - 1.54\n",
      "1.7 - 1.54\n",
      "1.91 - 1.54\n",
      "1.38 - 1.55\n",
      "2.14 - 1.55\n",
      "1.1 - 1.53\n",
      "0.986 - 1.52\n",
      "1.99 - 1.55\n",
      "1.31 - 1.52\n",
      "0.816 - 1.49\n",
      "2.75 - 1.58\n",
      "1.22 - 1.52\n",
      "1.18 - 1.55\n",
      "1.7 - 1.55\n",
      "1.16 - 1.54\n",
      "0.958 - 1.53\n",
      "1.3 - 1.52\n",
      "1.25 - 1.53\n",
      "1.61 - 1.54\n",
      "2.45 - 1.56\n",
      "1.66 - 1.57\n",
      "1.68 - 1.57\n",
      "0.921 - 1.51\n",
      "1.52 - 1.55\n",
      "1.18 - 1.53\n",
      "1.74 - 1.56\n",
      "2.23 - 1.56\n",
      "0.992 - 1.53\n",
      "2.35 - 1.58\n",
      "1.54 - 1.53\n",
      "1.18 - 1.49\n",
      "1.72 - 1.55\n",
      "0.859 - 1.52\n",
      "0.848 - 1.52\n",
      "2.74 - 1.59\n",
      "1.13 - 1.51\n",
      "1.01 - 1.53\n",
      "1.4 - 1.53\n",
      "1.65 - 1.56\n",
      "1.3 - 1.53\n",
      "1.42 - 1.52\n",
      "0.967 - 1.53\n",
      "1.38 - 1.54\n",
      "1.17 - 1.52\n",
      "2.15 - 1.54\n",
      "1.2 - 1.52\n",
      "1.74 - 1.53\n",
      "1.36 - 1.54\n",
      "1.56 - 1.54\n",
      "1.41 - 1.52\n",
      "0.798 - 1.49\n",
      "2.2 - 1.56\n",
      "2.02 - 1.59\n",
      "1.38 - 1.54\n",
      "1.91 - 1.53\n",
      "1.12 - 1.51\n",
      "1.36 - 1.53\n",
      "0.885 - 1.52\n",
      "1.96 - 1.55\n",
      "1.7 - 1.55\n",
      "1.26 - 1.52\n",
      "2.54 - 1.57\n",
      "2.52 - 1.6\n",
      "1.38 - 1.53\n",
      "0.584 - 1.51\n",
      "0.995 - 1.52\n",
      "1.85 - 1.54\n",
      "1.41 - 1.52\n",
      "1.4 - 1.53\n",
      "0.441 - 1.51\n",
      "2.06 - 1.56\n",
      "1.68 - 1.55\n",
      "1.23 - 1.53\n",
      "1.97 - 1.53\n",
      "0.741 - 1.52\n",
      "1.51 - 1.55\n",
      "1.87 - 1.53\n",
      "2.25 - 1.55\n",
      "1.69 - 1.53\n",
      "0.59 - 1.51\n",
      "1.33 - 1.53\n",
      "1.52 - 1.55\n",
      "0.992 - 1.52\n",
      "2.35 - 1.55\n",
      "0.854 - 1.53\n",
      "1.65 - 1.55\n",
      "0.138 - 1.51\n",
      "1.04 - 1.52\n",
      "0.973 - 1.53\n",
      "1.69 - 1.53\n",
      "1.54 - 1.54\n",
      "1.6 - 1.57\n",
      "1.74 - 1.52\n",
      "1.79 - 1.54\n",
      "1.14 - 1.53\n",
      "1.4 - 1.57\n",
      "1.16 - 1.53\n",
      "1.05 - 1.52\n",
      "1.66 - 1.53\n",
      "1.28 - 1.53\n",
      "1.13 - 1.52\n",
      "1.59 - 1.52\n",
      "1.17 - 1.52\n",
      "1.72 - 1.53\n",
      "7.0 - 1.8\n",
      "1.59 - 1.53\n",
      "3.54 - 1.59\n",
      "2.67 - 1.6\n",
      "1.57 - 1.53\n",
      "1.69 - 1.53\n",
      "1.43 - 1.52\n",
      "1.54 - 1.52\n",
      "1.2 - 1.54\n",
      "0.822 - 1.5\n",
      "1.51 - 1.54\n",
      "2.38 - 1.58\n",
      "1.19 - 1.55\n",
      "2.37 - 1.58\n",
      "2.0 - 1.54\n",
      "0.815 - 1.52\n",
      "1.55 - 1.52\n",
      "1.31 - 1.53\n",
      "1.18 - 1.53\n",
      "1.65 - 1.53\n",
      "6.38 - 1.69\n",
      "1.95 - 1.55\n",
      "2.45 - 1.57\n",
      "0.97 - 1.52\n",
      "1.92 - 1.55\n",
      "1.52 - 1.54\n",
      "1.9 - 1.54\n",
      "1.67 - 1.53\n",
      "1.86 - 1.54\n",
      "1.8 - 1.53\n",
      "1.77 - 1.57\n",
      "1.55 - 1.54\n",
      "1.13 - 1.52\n",
      "1.8 - 1.54\n",
      "1.44 - 1.56\n",
      "0.997 - 1.52\n",
      "1.36 - 1.51\n",
      "1.6 - 1.52\n",
      "0.876 - 1.51\n",
      "1.52 - 1.52\n",
      "0.902 - 1.51\n",
      "1.07 - 1.53\n",
      "1.66 - 1.55\n",
      "1.76 - 1.55\n",
      "1.35 - 1.52\n",
      "0.842 - 1.51\n",
      "1.77 - 1.57\n",
      "1.46 - 1.53\n",
      "0.856 - 1.52\n",
      "1.26 - 1.52\n",
      "2.57 - 1.57\n",
      "1.84 - 1.55\n",
      "1.47 - 1.54\n",
      "1.09 - 1.56\n",
      "1.28 - 1.52\n",
      "2.18 - 1.55\n",
      "0.991 - 1.51\n",
      "1.3 - 1.53\n",
      "1.36 - 1.54\n",
      "0.897 - 1.52\n",
      "1.48 - 1.53\n",
      "1.98 - 1.54\n",
      "0.128 - 1.5\n",
      "1.82 - 1.55\n",
      "1.25 - 1.53\n",
      "2.0 - 1.54\n",
      "1.33 - 1.53\n",
      "1.1 - 1.53\n",
      "1.28 - 1.54\n",
      "1.42 - 1.53\n",
      "1.1 - 1.51\n",
      "2.03 - 1.54\n",
      "1.71 - 1.52\n",
      "1.39 - 1.52\n",
      "6.43 - 1.8\n",
      "0.895 - 1.53\n",
      "1.86 - 1.58\n",
      "1.21 - 1.52\n",
      "1.05 - 1.52\n",
      "2.21 - 1.55\n",
      "1.48 - 1.51\n",
      "2.24 - 1.55\n",
      "1.72 - 1.54\n",
      "1.39 - 1.52\n",
      "1.54 - 1.55\n",
      "1.28 - 1.52\n",
      "2.53 - 1.59\n",
      "1.9 - 1.53\n",
      "1.74 - 1.54\n",
      "2.55 - 1.6\n",
      "0.972 - 1.52\n",
      "2.18 - 1.56\n",
      "1.14 - 1.53\n",
      "1.37 - 1.53\n",
      "1.43 - 1.52\n",
      "2.04 - 1.55\n",
      "2.49 - 1.56\n",
      "1.41 - 1.52\n",
      "0.714 - 1.52\n",
      "1.7 - 1.55\n",
      "0.585 - 1.51\n",
      "1.43 - 1.53\n",
      "2.05 - 1.56\n",
      "0.798 - 1.51\n",
      "2.06 - 1.56\n",
      "0.502 - 1.5\n",
      "1.71 - 1.54\n",
      "2.34 - 1.58\n",
      "1.37 - 1.53\n",
      "0.923 - 1.52\n",
      "0.735 - 1.51\n",
      "1.46 - 1.53\n",
      "2.2 - 1.57\n",
      "1.05 - 1.52\n",
      "0.953 - 1.52\n",
      "0.582 - 1.51\n",
      "0.858 - 1.53\n",
      "1.54 - 1.52\n",
      "1.38 - 1.53\n",
      "0.754 - 1.52\n",
      "1.26 - 1.55\n",
      "2.18 - 1.55\n",
      "1.65 - 1.53\n",
      "7.17 - 1.8\n",
      "1.86 - 1.55\n",
      "1.69 - 1.58\n",
      "1.28 - 1.54\n",
      "1.18 - 1.53\n",
      "0.941 - 1.52\n",
      "0.915 - 1.52\n",
      "2.09 - 1.54\n",
      "1.14 - 1.52\n",
      "2.21 - 1.57\n",
      "0.423 - 1.5\n",
      "1.19 - 1.53\n",
      "1.42 - 1.52\n",
      "1.29 - 1.54\n",
      "1.19 - 1.51\n",
      "1.06 - 1.52\n",
      "1.77 - 1.57\n",
      "1.44 - 1.53\n",
      "1.87 - 1.53\n",
      "1.52 - 1.53\n",
      "1.88 - 1.55\n",
      "1.96 - 1.55\n",
      "0.649 - 1.52\n",
      "1.74 - 1.55\n",
      "0.894 - 1.52\n",
      "2.35 - 1.57\n",
      "5.56 - 1.85\n",
      "1.11 - 1.52\n",
      "1.95 - 1.57\n",
      "1.5 - 1.54\n",
      "0.98 - 1.53\n",
      "1.09 - 1.52\n",
      "1.08 - 1.52\n",
      "0.985 - 1.52\n",
      "2.04 - 1.54\n",
      "2.41 - 1.58\n",
      "1.44 - 1.55\n",
      "2.21 - 1.56\n",
      "0.552 - 1.52\n",
      "1.86 - 1.54\n",
      "1.66 - 1.55\n",
      "3.31 - 1.62\n",
      "2.2 - 1.54\n",
      "2.21 - 1.58\n",
      "1.97 - 1.54\n",
      "2.87 - 1.58\n",
      "1.13 - 1.53\n",
      "2.13 - 1.56\n",
      "1.52 - 1.53\n",
      "2.49 - 1.55\n",
      "1.25 - 1.51\n",
      "1.45 - 1.53\n",
      "1.78 - 1.54\n",
      "1.16 - 1.53\n",
      "1.23 - 1.52\n",
      "2.59 - 1.58\n",
      "1.31 - 1.52\n",
      "3.59 - 1.61\n",
      "0.255 - 1.5\n",
      "0.939 - 1.52\n",
      "1.82 - 1.56\n",
      "0.909 - 1.52\n",
      "1.11 - 1.53\n",
      "0.51 - 1.51\n",
      "0.23 - 1.51\n",
      "0.961 - 1.52\n",
      "1.31 - 1.56\n",
      "1.38 - 1.54\n",
      "0.289 - 1.51\n",
      "1.39 - 1.54\n",
      "0.542 - 1.51\n",
      "1.75 - 1.55\n",
      "1.71 - 1.54\n",
      "2.62 - 1.57\n",
      "1.52 - 1.52\n",
      "1.3 - 1.54\n",
      "2.26 - 1.55\n",
      "1.98 - 1.55\n",
      "1.48 - 1.52\n",
      "1.38 - 1.53\n",
      "1.43 - 1.54\n",
      "1.3 - 1.53\n",
      "0.548 - 1.5\n",
      "1.59 - 1.53\n",
      "1.79 - 1.53\n",
      "1.79 - 1.55\n",
      "1.26 - 1.52\n",
      "0.773 - 1.52\n",
      "0.635 - 1.51\n",
      "0.806 - 1.51\n",
      "2.41 - 1.58\n",
      "1.81 - 1.54\n",
      "1.96 - 1.53\n",
      "0.979 - 1.52\n",
      "1.07 - 1.53\n",
      "1.8 - 1.53\n",
      "1.57 - 1.53\n",
      "1.45 - 1.54\n",
      "1.28 - 1.52\n",
      "1.44 - 1.52\n",
      "1.04 - 1.53\n",
      "3.24 - 1.69\n",
      "0.612 - 1.51\n",
      "1.07 - 1.51\n",
      "0.945 - 1.51\n",
      "0.524 - 1.5\n",
      "1.71 - 1.54\n",
      "1.68 - 1.53\n",
      "0.426 - 1.5\n",
      "1.93 - 1.54\n",
      "2.28 - 1.57\n",
      "1.49 - 1.55\n",
      "1.62 - 1.54\n",
      "1.97 - 1.55\n",
      "1.33 - 1.53\n",
      "1.41 - 1.52\n",
      "1.88 - 1.54\n",
      "1.58 - 1.54\n",
      "1.54 - 1.53\n",
      "0.874 - 1.52\n",
      "1.21 - 1.51\n",
      "2.3 - 1.56\n",
      "2.34 - 1.58\n",
      "0.616 - 1.52\n",
      "1.1 - 1.52\n",
      "1.44 - 1.55\n",
      "1.71 - 1.53\n",
      "1.05 - 1.52\n",
      "1.93 - 1.55\n",
      "0.866 - 1.52\n",
      "1.05 - 1.53\n",
      "1.47 - 1.54\n",
      "1.76 - 1.56\n",
      "1.87 - 1.56\n",
      "2.01 - 1.54\n",
      "5.08 - 1.73\n",
      "1.79 - 1.53\n",
      "0.765 - 1.51\n",
      "1.3 - 1.52\n",
      "0.856 - 1.52\n",
      "1.3 - 1.54\n",
      "1.47 - 1.54\n",
      "1.71 - 1.54\n",
      "1.16 - 1.53\n",
      "1.27 - 1.53\n",
      "2.28 - 1.57\n",
      "1.26 - 1.53\n",
      "1.26 - 1.52\n",
      "1.49 - 1.53\n",
      "0.9 - 1.56\n",
      "1.16 - 1.54\n",
      "2.06 - 1.55\n",
      "1.17 - 1.52\n",
      "1.96 - 1.56\n",
      "0.636 - 1.51\n",
      "1.53 - 1.56\n",
      "2.16 - 1.58\n",
      "1.82 - 1.54\n",
      "1.21 - 1.52\n",
      "1.65 - 1.54\n",
      "2.02 - 1.55\n",
      "1.48 - 1.54\n",
      "1.18 - 1.54\n",
      "1.44 - 1.53\n",
      "1.26 - 1.52\n",
      "1.17 - 1.53\n",
      "2.09 - 1.56\n",
      "0.761 - 1.49\n",
      "1.22 - 1.52\n",
      "1.11 - 1.53\n",
      "0.957 - 1.51\n",
      "4.73 - 1.56\n",
      "1.49 - 1.54\n",
      "1.47 - 1.53\n",
      "7.01 - 1.8\n",
      "1.65 - 1.53\n",
      "0.926 - 1.52\n",
      "2.07 - 1.57\n",
      "1.38 - 1.54\n",
      "0.878 - 1.52\n",
      "1.52 - 1.54\n",
      "0.97 - 1.52\n",
      "0.911 - 1.53\n",
      "1.34 - 1.52\n",
      "1.56 - 1.52\n",
      "1.81 - 1.55\n",
      "2.88 - 1.57\n",
      "0.485 - 1.5\n",
      "0.651 - 1.51\n",
      "1.61 - 1.52\n",
      "0.618 - 1.52\n",
      "0.803 - 1.51\n",
      "1.17 - 1.53\n",
      "2.35 - 1.59\n",
      "1.06 - 1.52\n",
      "1.23 - 1.52\n",
      "0.664 - 1.52\n",
      "1.91 - 1.54\n",
      "0.764 - 1.52\n",
      "2.39 - 1.57\n",
      "0.753 - 1.51\n",
      "2.03 - 1.55\n",
      "1.26 - 1.52\n",
      "2.01 - 1.54\n",
      "2.38 - 1.55\n",
      "1.5 - 1.55\n",
      "2.8 - 1.6\n",
      "2.44 - 1.57\n",
      "1.68 - 1.54\n",
      "0.769 - 1.52\n",
      "0.687 - 1.53\n",
      "1.45 - 1.53\n",
      "0.229 - 1.5\n",
      "1.55 - 1.55\n"
     ]
    }
   ],
   "source": [
    "print \"Test Results for Time Regression\"\n",
    "predict(X2_test, y2_test, time_model, mse_criterion, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
