{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import system\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals.joblib import dump, load\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], 'datagen/img_generator/'))\n",
    "from network import Net, Net_Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of data:  (8619, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_init_x</th>\n",
       "      <th>true_init_y</th>\n",
       "      <th>true_init_z</th>\n",
       "      <th>jitter_coef</th>\n",
       "      <th>var_sum</th>\n",
       "      <th>diff_x</th>\n",
       "      <th>diff_y</th>\n",
       "      <th>diff_z</th>\n",
       "      <th>diff_phi</th>\n",
       "      <th>diff_theta</th>\n",
       "      <th>diff_psi</th>\n",
       "      <th>r_std</th>\n",
       "      <th>phi_std</th>\n",
       "      <th>theta_std</th>\n",
       "      <th>psi_std</th>\n",
       "      <th>Tf</th>\n",
       "      <th>MP_Method</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.104533</td>\n",
       "      <td>-0.018855</td>\n",
       "      <td>-2.245773</td>\n",
       "      <td>2.780655</td>\n",
       "      <td>0.212268</td>\n",
       "      <td>0.057597</td>\n",
       "      <td>-3.303025</td>\n",
       "      <td>0.143396</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>0.814931</td>\n",
       "      <td>0.172320</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>0.030144</td>\n",
       "      <td>2.561507</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>666.401116</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.118917</td>\n",
       "      <td>-0.371141</td>\n",
       "      <td>-2.239867</td>\n",
       "      <td>10.727064</td>\n",
       "      <td>0.720797</td>\n",
       "      <td>0.079548</td>\n",
       "      <td>-1.481881</td>\n",
       "      <td>0.141947</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>-0.414747</td>\n",
       "      <td>0.694450</td>\n",
       "      <td>0.011473</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>1.209763</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>139.277568</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.125599</td>\n",
       "      <td>-0.698408</td>\n",
       "      <td>-2.228797</td>\n",
       "      <td>10.727064</td>\n",
       "      <td>0.033839</td>\n",
       "      <td>-0.633924</td>\n",
       "      <td>-0.945962</td>\n",
       "      <td>1.452816</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>0.470498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005313</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.026794</td>\n",
       "      <td>1.474120</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>244.203045</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.087731</td>\n",
       "      <td>-1.025364</td>\n",
       "      <td>-2.148764</td>\n",
       "      <td>10.727064</td>\n",
       "      <td>0.171747</td>\n",
       "      <td>-0.367246</td>\n",
       "      <td>-1.142579</td>\n",
       "      <td>-0.304712</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>0.299181</td>\n",
       "      <td>0.138481</td>\n",
       "      <td>0.006888</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.024089</td>\n",
       "      <td>1.021753</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>109.635576</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.013654</td>\n",
       "      <td>-1.178420</td>\n",
       "      <td>-2.078171</td>\n",
       "      <td>10.727064</td>\n",
       "      <td>0.459846</td>\n",
       "      <td>-0.252224</td>\n",
       "      <td>-1.408724</td>\n",
       "      <td>-0.318564</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>-0.107583</td>\n",
       "      <td>0.431646</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.018747</td>\n",
       "      <td>1.191425</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>153.026475</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.105886</td>\n",
       "      <td>-1.429338</td>\n",
       "      <td>-2.044954</td>\n",
       "      <td>10.727064</td>\n",
       "      <td>0.024708</td>\n",
       "      <td>-0.043336</td>\n",
       "      <td>-0.706722</td>\n",
       "      <td>0.232820</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>0.080075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008324</td>\n",
       "      <td>0.012506</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.654845</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>44.141688</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.135498</td>\n",
       "      <td>-1.594819</td>\n",
       "      <td>-2.034136</td>\n",
       "      <td>10.727064</td>\n",
       "      <td>0.257183</td>\n",
       "      <td>-0.040947</td>\n",
       "      <td>-1.477477</td>\n",
       "      <td>0.548405</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>-0.226680</td>\n",
       "      <td>0.219388</td>\n",
       "      <td>0.007363</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.020261</td>\n",
       "      <td>1.273571</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>169.766987</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.160248</td>\n",
       "      <td>-1.900392</td>\n",
       "      <td>-1.990043</td>\n",
       "      <td>1.880817</td>\n",
       "      <td>1.195650</td>\n",
       "      <td>0.308336</td>\n",
       "      <td>-2.382304</td>\n",
       "      <td>-0.196460</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>-0.059965</td>\n",
       "      <td>1.136596</td>\n",
       "      <td>0.008237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050817</td>\n",
       "      <td>1.894183</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>442.897805</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.134419</td>\n",
       "      <td>-2.413613</td>\n",
       "      <td>-1.939049</td>\n",
       "      <td>1.880817</td>\n",
       "      <td>1.102345</td>\n",
       "      <td>0.182406</td>\n",
       "      <td>-2.400703</td>\n",
       "      <td>0.023451</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>-0.061924</td>\n",
       "      <td>1.046051</td>\n",
       "      <td>0.005625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050669</td>\n",
       "      <td>1.892353</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>421.933112</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.064802</td>\n",
       "      <td>-2.988882</td>\n",
       "      <td>-1.924189</td>\n",
       "      <td>1.880817</td>\n",
       "      <td>0.251579</td>\n",
       "      <td>0.266086</td>\n",
       "      <td>-1.512065</td>\n",
       "      <td>-0.201828</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.049895</td>\n",
       "      <td>-0.381524</td>\n",
       "      <td>0.242358</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.252730</td>\n",
       "      <td>min_vel</td>\n",
       "      <td>155.128768</td>\n",
       "      <td>SUCCESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_init_x  true_init_y  true_init_z  jitter_coef   var_sum    diff_x  \\\n",
       "0     0.104533    -0.018855    -2.245773     2.780655  0.212268  0.057597   \n",
       "1     0.118917    -0.371141    -2.239867    10.727064  0.720797  0.079548   \n",
       "2     0.125599    -0.698408    -2.228797    10.727064  0.033839 -0.633924   \n",
       "3     0.087731    -1.025364    -2.148764    10.727064  0.171747 -0.367246   \n",
       "4    -0.013654    -1.178420    -2.078171    10.727064  0.459846 -0.252224   \n",
       "5    -0.105886    -1.429338    -2.044954    10.727064  0.024708 -0.043336   \n",
       "6    -0.135498    -1.594819    -2.034136    10.727064  0.257183 -0.040947   \n",
       "7    -0.160248    -1.900392    -1.990043     1.880817  1.195650  0.308336   \n",
       "8    -0.134419    -2.413613    -1.939049     1.880817  1.102345  0.182406   \n",
       "9    -0.064802    -2.988882    -1.924189     1.880817  0.251579  0.266086   \n",
       "\n",
       "     diff_y    diff_z  diff_phi  diff_theta  diff_psi     r_std   phi_std  \\\n",
       "0 -3.303025  0.143396  0.055103    0.049895  0.814931  0.172320  0.003544   \n",
       "1 -1.481881  0.141947  0.055103    0.049895 -0.414747  0.694450  0.011473   \n",
       "2 -0.945962  1.452816  0.055103    0.049895  0.470498  0.000000  0.005313   \n",
       "3 -1.142579 -0.304712  0.055103    0.049895  0.299181  0.138481  0.006888   \n",
       "4 -1.408724 -0.318564  0.055103    0.049895 -0.107583  0.431646  0.007526   \n",
       "5 -0.706722  0.232820  0.055103    0.049895  0.080075  0.000000  0.008324   \n",
       "6 -1.477477  0.548405  0.055103    0.049895 -0.226680  0.219388  0.007363   \n",
       "7 -2.382304 -0.196460  0.055103    0.049895 -0.059965  1.136596  0.008237   \n",
       "8 -2.400703  0.023451  0.055103    0.049895 -0.061924  1.046051  0.005625   \n",
       "9 -1.512065 -0.201828  0.055103    0.049895 -0.381524  0.242358  0.002321   \n",
       "\n",
       "   theta_std   psi_std        Tf MP_Method        Cost   Status  \n",
       "0   0.006260  0.030144  2.561507   min_vel  666.401116  SUCCESS  \n",
       "1   0.001152  0.013722  1.209763   min_vel  139.277568  SUCCESS  \n",
       "2   0.001731  0.026794  1.474120   min_vel  244.203045  SUCCESS  \n",
       "3   0.002289  0.024089  1.021753   min_vel  109.635576  SUCCESS  \n",
       "4   0.001927  0.018747  1.191425   min_vel  153.026475  SUCCESS  \n",
       "5   0.012506  0.003878  0.654845   min_vel   44.141688  SUCCESS  \n",
       "6   0.010171  0.020261  1.273571   min_vel  169.766987  SUCCESS  \n",
       "7   0.000000  0.050817  1.894183   min_vel  442.897805  SUCCESS  \n",
       "8   0.000000  0.050669  1.892353   min_vel  421.933112  SUCCESS  \n",
       "9   0.000000  0.006900  1.252730   min_vel  155.128768  SUCCESS  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('classifier_files/data.csv')\n",
    "print \"Total length of data: \", data.shape\n",
    "data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution (Before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfull flights: 93.4 % \n",
      "Crashed flights: 0.29 % \n",
      "Out of path flights: 0.29 % \n",
      "Collided flights: 5.98 % \n",
      "\n",
      "MP Method distribution, before any restricions\n",
      "Rate of min_vel method in Flights: 25.6 % \n",
      "Rate of min_acc method in Flights: 24.8 % \n",
      "Rate of min_jerk method in Flights: 19.6 % \n",
      "Rate of min_jerk_full_stop method in Flights: 23.4 % \n",
      "Rate of safe_mode in Flights 6.56\n"
     ]
    }
   ],
   "source": [
    "print \"Successfull flights: {0:.3} % \".format(len(data[data[\"Status\"] == \"SUCCESS\"]) / float(len(data)) * 100)\n",
    "print \"Crashed flights: {0:.3} % \".format(len(data[data[\"Status\"] == \"CRASH\"]) / float(len(data)) * 100)\n",
    "print \"Out of path flights: {0:.3} % \".format(len(data[data[\"Status\"] == \"OFF_ROAD\"]) / float(len(data)) * 100)\n",
    "print \"Collided flights: {0:.3} % \".format(len(data[data[\"Status\"] == \"COLLISION\"]) / float(len(data)) * 100)\n",
    "\n",
    "print \"\\nMP Method distribution, before any restricions\"\n",
    "print \"Rate of min_vel method in Flights: {0:.3} % \".format(len(data[(data[\"MP_Method\"] == \"min_vel\") & (data[\"Status\"] == \"SUCCESS\")]) / float(len(data)) * 100)\n",
    "print \"Rate of min_acc method in Flights: {0:.3} % \".format(len(data[(data[\"MP_Method\"] == \"min_acc\") & (data[\"Status\"] == \"SUCCESS\")]) / float(len(data)) * 100)\n",
    "print \"Rate of min_jerk method in Flights: {0:.3} % \".format(len(data[(data[\"MP_Method\"] == \"min_jerk\") & (data[\"Status\"] == \"SUCCESS\")]) / float(len(data)) * 100)\n",
    "print \"Rate of min_jerk_full_stop method in Flights: {0:.3} % \".format(len(data[(data[\"MP_Method\"] == \"min_jerk_full_stop\") & (data[\"Status\"] == \"SUCCESS\")]) / float(len(data)) * 100)\n",
    "print \"Rate of safe_mode in Flights {0:.3}\".format(len(data[data[\"Status\"] != \"SUCCESS\"]) / float(len(data)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution (After)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of flight points, whose cost is below 1.0*median: 50.0 %\n",
      "\n",
      "Data distribution, after median value restriction\n",
      "Rate of min_vel method in Flights: 27.4 % \n",
      "Rate of min_acc method in Flights: 29.3 % \n",
      "Rate of min_jerk method in Flights: 18.2 % \n",
      "Rate of min_jerk_full_stop method in Flights: 25.1 % \n",
      "Upper cost limit: 340.883\n"
     ]
    }
   ],
   "source": [
    "data = data.sort_values('Cost')\n",
    "median_cost = np.median(data[\"Cost\"])\n",
    "mean_cost = np.mean(data[\"Cost\"])\n",
    "\n",
    "data_coeff = 1.\n",
    "median_data = data[data[\"Cost\"] <= data_coeff*median_cost]\n",
    "unsucessful_flights = data[data[\"Status\"] != \"SUCCESS\"]\n",
    "\n",
    "print \"Percentage of flight points, whose cost is below {0:.3}*median: {1:.3} %\".format(data_coeff, 100. * len(median_data) / len(data))\n",
    "print \"\\nData distribution, after median value restriction\"\n",
    "print \"Rate of min_vel method in Flights: {0:.3} % \".format(len(median_data[median_data[\"MP_Method\"] == \"min_vel\"]) / float(len(median_data)) * 100)\n",
    "print \"Rate of min_acc method in Flights: {0:.3} % \".format(len(median_data[median_data[\"MP_Method\"] == \"min_acc\"]) / float(len(median_data)) * 100)\n",
    "print \"Rate of min_jerk method in Flights: {0:.3} % \".format(len(median_data[median_data[\"MP_Method\"] == \"min_jerk\"]) / float(len(median_data)) * 100)\n",
    "print \"Rate of min_jerk_full_stop method in Flights: {0:.3} % \".format(len(median_data[median_data[\"MP_Method\"] == \"min_jerk_full_stop\"]) / float(len(median_data)) * 100)\n",
    "print \"Upper cost limit: {0:.6}\".format(data_coeff*median_cost)\n",
    "\n",
    "median_arr = median_data.values\n",
    "unsuccessful_arr = unsucessful_flights.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dataset variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_dict = {\"min_vel\":0, \"min_acc\":1, \"min_jerk\":2, \"min_jerk_full_stop\":3}\n",
    "algorithms = [\"min_vel\", \"min_acc\", \"min_jerk\", \"min_jerk_full_stop\"]\n",
    "\n",
    "successful_data = np.copy(median_data.values)\n",
    "unsuccessful_data = np.copy(unsucessful_flights.values)\n",
    "for i, data in enumerate(median_data.values):\n",
    "    successful_data[i][16] = algorithm_dict[data[16]]\n",
    "    \n",
    "for i, data in enumerate(unsucessful_flights.values):\n",
    "    unsuccessful_data[i][16] = 4\n",
    "    \n",
    "successful_classif = np.c_[successful_data[:,5:11], successful_data[:,16]]\n",
    "unsuccessful_classif = np.c_[unsuccessful_data[:,5:11], unsuccessful_data[:,16]]\n",
    "successful_regressor = np.c_[successful_data[:,5:11], successful_data[:,15]]\n",
    "unsuccessful_regressor = np.c_[unsuccessful_data[:,5:11], unsuccessful_data[:,15]]\n",
    "\n",
    "classification_dataset = np.r_[successful_classif, unsuccessful_classif]\n",
    "regression_dataset = np.r_[successful_regressor, unsuccessful_regressor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP Dataset size:  (4875, 7)\n",
      "Regressor Dataset size:  (4875, 7)\n",
      "N_min_vel: 24.25\n",
      "N_min_acc: 25.87\n",
      "N_min_jerk: 16.12\n",
      "N_min_jerk_stop: 22.17\n",
      "N_safe_mode: 11.59\n"
     ]
    }
   ],
   "source": [
    "class_labels = {\"min_vel\":0, \"min_acc\":1, \"min_jerk\":2, \"min_jerk_full_stop\":3, \"safe_mode\":4}\n",
    "N_length = float(classification_dataset.shape[0])\n",
    "print \"MP Dataset size: \",classification_dataset.shape\n",
    "print \"Regressor Dataset size: \",regression_dataset.shape \n",
    "print \"N_min_vel: {0:.4}\".format(np.sum(classification_dataset[:,-1] == class_labels[\"min_vel\"]) / N_length * 100)\n",
    "print \"N_min_acc: {0:.4}\".format(np.sum(classification_dataset[:,-1] == class_labels[\"min_acc\"]) / N_length * 100)\n",
    "print \"N_min_jerk: {0:.4}\".format(np.sum(classification_dataset[:,-1] == class_labels[\"min_jerk\"]) / N_length * 100)\n",
    "print \"N_min_jerk_stop: {0:.4}\".format(np.sum(classification_dataset[:,-1] == class_labels[\"min_jerk_full_stop\"]) / N_length * 100)\n",
    "print \"N_safe_mode: {0:.4}\".format(np.sum(classification_dataset[:,-1] == class_labels[\"safe_mode\"]) / N_length * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split of dataset as train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1_train: (3500, 6) X2_train: (3500, 6)\n",
      "X1_val: (875, 6) X2_val: (875, 6)\n",
      "X1_test: (500, 6) X2_test: (500, 6)\n"
     ]
    }
   ],
   "source": [
    "N_Test = 500\n",
    "np.random.shuffle(classification_dataset) # Train 1\n",
    "np.random.shuffle(regression_dataset) # Train 2\n",
    "\n",
    "y1 = np.array(classification_dataset[:,-1],dtype=np.int)\n",
    "y2 = np.array(regression_dataset[:,-1],dtype=np.float)\n",
    "X1 = classification_dataset[:,:-1]\n",
    "X2 = regression_dataset[:,:-1]\n",
    "\n",
    "X1_test = X1[0:N_Test,:]\n",
    "y1_test = y1[0:N_Test]\n",
    "X2_test = X2[0:N_Test,:]\n",
    "y2_test = y2[0:N_Test]\n",
    "\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(X1[N_Test:,:], y1[N_Test:], test_size=0.2, random_state=42)\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(X2[N_Test:,:], y2[N_Test:], test_size=0.2, random_state=42)\n",
    "    \n",
    "pickle.dump([X1_train, X1_val, X1_test, y1_train, y1_val, y1_test], open(\"classifier_files/dataset1.pkl\",\"wb\"), protocol=2)\n",
    "pickle.dump([X2_train, X2_val, X2_test, y2_train, y2_val, y2_test], open(\"classifier_files/dataset2.pkl\",\"wb\"), protocol=2)\n",
    "\n",
    "scaler1,scaler2 = StandardScaler(), StandardScaler()\n",
    "scaler1.fit(X1_train)\n",
    "scaler2.fit(X2_train)\n",
    "X1_train, X2_train = scaler1.transform(X1_train), scaler2.transform(X2_train)\n",
    "X1_val, X2_val = scaler1.transform(X1_val), scaler2.transform(X2_val)\n",
    "X1_test, X2_test = scaler1.transform(X1_test), scaler2.transform(X2_test)\n",
    "\n",
    "dump(scaler1, 'classifier_files/mp_scaler.bin', compress=True)\n",
    "dump(scaler2, 'classifier_files/time_scaler.bin', compress=True)\n",
    "\n",
    "print \"X1_train: \" + str(X1_train.shape) + \" X2_train: \" + str(X2_train.shape)\n",
    "print \"X1_val: \" + str(X1_val.shape) + \" X2_val: \" + str(X2_val.shape) \n",
    "print \"X1_test: \" + str(X1_test.shape) + \" X2_test: \" + str(X2_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, model, criterion, isClassifier):\n",
    "    #Validation part\n",
    "    model.eval()  # Set model to training mode\n",
    "    \n",
    "    inputs = torch.from_numpy(X).to(device)\n",
    "    if isClassifier:\n",
    "        labels = torch.from_numpy(y).to(device).long()\n",
    "    else:\n",
    "        labels = torch.from_numpy(y).to(device).float()\n",
    "\n",
    "    outputs = model(inputs.float())\n",
    "    if isClassifier:\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    if isClassifier:\n",
    "        accuracy = torch.sum(preds == labels.data).item() / float(inputs.size(0))\n",
    "        print \"Test data, Loss: {0:.3}, Accuracy: {1:.4}\".format(loss.item(), accuracy)\n",
    "    else:\n",
    "        print \"Test data, Loss: {0:.3}\\n\".format(loss.item())\n",
    "        print \"Actual - Predicted\"\n",
    "        pred = outputs.detach().cpu().numpy()\n",
    "        for i in range(y.shape[0]):\n",
    "            print \"{0:.3} - {1:.3}\".format(y[i], outputs[i].item())\n",
    "        \n",
    "    \n",
    "\n",
    "def shuffle_dataset(X, y):\n",
    "    p = np.random.permutation(len(X))\n",
    "    return X[p], y[p]\n",
    "\n",
    "def train_model(isClassifier, X, y, X_val, y_val, model, criterion, optimizer, scheduler, minibatch_size, name, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_train_acc = 0.0\n",
    "    best_acc = 0.0\n",
    "    best_loss = 1e6\n",
    "    best_train_loss = 1e6\n",
    "    \n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    accuracy_train = []\n",
    "    accuracy_val = []\n",
    "    # path = F\"/content/drive/My Drive/best_model.pt\"\n",
    "#     directory = path_name\n",
    "\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        X_train, y_train = shuffle_dataset(X, y)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        losses_iter = []\n",
    "        accuracy_iter = []\n",
    "\n",
    "        # Iterate over data.\n",
    "        for i in range(0, X_train.shape[0], minibatch_size):\n",
    "            # Get pair of (X, y) of the current minibatch/chunk             \n",
    "            X_batch = X_train[i:i + minibatch_size]\n",
    "            y_batch = y_train[i:i + minibatch_size]\n",
    "\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            xbatch = torch.from_numpy(X_batch).to(device)\n",
    "            \n",
    "            if isClassifier:\n",
    "                ybatch = torch.from_numpy(y_batch).to(device).long()\n",
    "            else:\n",
    "                ybatch = torch.from_numpy(y_batch).to(device).float()\n",
    "\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                pred = model(xbatch.float())\n",
    "                if isClassifier:\n",
    "                    _, preds = torch.max(pred, 1)\n",
    "                \n",
    "                loss = criterion(pred, ybatch)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            # statistics\n",
    "            running_loss += loss.item() * xbatch.size(0)\n",
    "            if isClassifier:\n",
    "                running_corrects += torch.sum(preds == ybatch.data)\n",
    "\n",
    "            # print (\"losses_iter\", loss.item() * inputs.size(0))\n",
    "            # print (\"accuracy_iter\", torch.sum(preds == labels.data).item() / float(inputs.size(0)))\n",
    "\n",
    "            losses_iter.append(loss.item())\n",
    "            if isClassifier:\n",
    "                accuracy_iter.append(torch.sum(preds == ybatch.data).item() / float(xbatch.size(0)))\n",
    "        \n",
    "        \n",
    "        train_loss = np.mean(losses_iter)\n",
    "        losses_train.append(train_loss)\n",
    "        if isClassifier:\n",
    "            train_acc = np.mean(accuracy_iter)\n",
    "            accuracy_train.append(train_acc)\n",
    "            print'Training Loss: {:.4f} Acc: {:.4f}'.format(train_loss, train_acc)  \n",
    "        else:\n",
    "            print'Training Loss: {:.4f}'.format(train_loss)\n",
    " \n",
    "        \n",
    "        \n",
    "        #Validation part\n",
    "        model.eval()  # Set model to training mode\n",
    "        \n",
    "        xbatch = torch.from_numpy(X_val).to(device)\n",
    "        if isClassifier:\n",
    "            ybatch = torch.from_numpy(y_val).to(device).long()\n",
    "        else:\n",
    "            ybatch = torch.from_numpy(y_val).to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(xbatch.float())\n",
    "        if isClassifier:\n",
    "            _, preds = torch.max(pred, 1)\n",
    "\n",
    "        loss = criterion(pred, ybatch)\n",
    "        \n",
    "        val_loss = loss.item()\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if isClassifier:\n",
    "            val_acc = torch.sum(preds == ybatch.data).item() / float(xbatch.size(0))\n",
    "            accuracy_val.append(val_acc)\n",
    "        \n",
    "        losses_val.append(val_loss)\n",
    "        \n",
    "        \n",
    "        if isClassifier:\n",
    "            print'Validation Loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_acc)\n",
    "        else:\n",
    "            print'Validation Loss: {:.4f}'.format(val_loss)\n",
    "\n",
    "#         deep copy the model\n",
    "        if isClassifier:\n",
    "            if val_acc > best_acc:\n",
    "                best_train_acc = train_acc\n",
    "                best_acc = val_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                path = name + \"_best_model.pt\"\n",
    "                torch.save(best_model_wts, \"classifier_files/\" + path)\n",
    "        else:\n",
    "            if val_loss < best_loss:\n",
    "                best_train_loss = train_loss\n",
    "                best_loss = val_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                path = name + \"_best_model.pt\"\n",
    "                torch.save(best_model_wts, \"classifier_files/\" + path)\n",
    "\n",
    "        print \"\"\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print'Training complete in {:.0f}s'.format(time_elapsed)\n",
    "    if isClassifier:\n",
    "        print'Best Train Acc: {:4f}'.format(best_train_acc)\n",
    "        print'Best Val Acc: {:4f}'.format(best_acc)\n",
    "    else:\n",
    "        print'Best Train Loss: {:4f}'.format(best_train_loss)\n",
    "        print'Best Val Loss: {:4f}'.format(best_loss)\n",
    "\n",
    "    \n",
    "#     stats_columns = ['Layers', 'Epochs', 'BatchSize', 'LearningRate', 'Optimizer', 'Scheduler', 'TrainAcc', 'ValAcc']\n",
    "#     layers = [module for module in model.modules() if type(module) != nn.Sequential]\n",
    "    #write_results([layers, num_epochs, minibatch_size, learning_rate, optimizer.state_dict, scheduler.state_dict, best_train_acc, best_val_acc])\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MP Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "Training Loss: 1.4594 Acc: 0.3024\n",
      "Validation Loss: 1.4337 Acc: 0.3200\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "Training Loss: 1.4150 Acc: 0.3344\n",
      "Validation Loss: 1.4033 Acc: 0.3589\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "Training Loss: 1.4033 Acc: 0.3428\n",
      "Validation Loss: 1.4072 Acc: 0.3451\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "Training Loss: 1.3854 Acc: 0.3536\n",
      "Validation Loss: 1.3846 Acc: 0.3600\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "Training Loss: 1.3772 Acc: 0.3669\n",
      "Validation Loss: 1.3863 Acc: 0.3577\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "Training Loss: 1.3711 Acc: 0.3703\n",
      "Validation Loss: 1.3880 Acc: 0.3451\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "Training Loss: 1.3678 Acc: 0.3776\n",
      "Validation Loss: 1.3871 Acc: 0.3726\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "Training Loss: 1.3553 Acc: 0.3755\n",
      "Validation Loss: 1.3790 Acc: 0.3486\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "Training Loss: 1.3484 Acc: 0.3897\n",
      "Validation Loss: 1.3875 Acc: 0.3600\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "Training Loss: 1.3460 Acc: 0.3848\n",
      "Validation Loss: 1.3757 Acc: 0.3886\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "Training Loss: 1.3417 Acc: 0.3908\n",
      "Validation Loss: 1.3780 Acc: 0.3474\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "Training Loss: 1.3368 Acc: 0.3957\n",
      "Validation Loss: 1.3669 Acc: 0.3714\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "Training Loss: 1.3302 Acc: 0.4013\n",
      "Validation Loss: 1.3797 Acc: 0.3577\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "Training Loss: 1.3239 Acc: 0.4108\n",
      "Validation Loss: 1.3737 Acc: 0.3817\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "Training Loss: 1.3169 Acc: 0.4128\n",
      "Validation Loss: 1.3648 Acc: 0.3691\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "Training Loss: 1.3055 Acc: 0.4166\n",
      "Validation Loss: 1.3661 Acc: 0.3771\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "Training Loss: 1.3090 Acc: 0.4031\n",
      "Validation Loss: 1.3836 Acc: 0.3714\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "Training Loss: 1.2980 Acc: 0.4141\n",
      "Validation Loss: 1.3800 Acc: 0.3714\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "Training Loss: 1.2944 Acc: 0.4204\n",
      "Validation Loss: 1.3835 Acc: 0.3966\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "Training Loss: 1.2905 Acc: 0.4248\n",
      "Validation Loss: 1.3748 Acc: 0.3577\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "Training Loss: 1.2872 Acc: 0.4296\n",
      "Validation Loss: 1.3835 Acc: 0.3669\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "Training Loss: 1.2745 Acc: 0.4398\n",
      "Validation Loss: 1.3877 Acc: 0.3817\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "Training Loss: 1.2788 Acc: 0.4286\n",
      "Validation Loss: 1.3717 Acc: 0.3874\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "Training Loss: 1.2690 Acc: 0.4367\n",
      "Validation Loss: 1.3780 Acc: 0.3886\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "Training Loss: 1.2600 Acc: 0.4473\n",
      "Validation Loss: 1.3670 Acc: 0.4000\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "Training Loss: 1.2606 Acc: 0.4422\n",
      "Validation Loss: 1.3683 Acc: 0.3703\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "Training Loss: 1.2271 Acc: 0.4657\n",
      "Validation Loss: 1.3671 Acc: 0.3817\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "Training Loss: 1.2228 Acc: 0.4721\n",
      "Validation Loss: 1.3711 Acc: 0.3829\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "Training Loss: 1.2192 Acc: 0.4697\n",
      "Validation Loss: 1.3723 Acc: 0.3829\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "Training Loss: 1.2207 Acc: 0.4704\n",
      "Validation Loss: 1.3751 Acc: 0.3863\n",
      "\n",
      "Training complete in 12s\n",
      "Best Train Acc: 0.447298\n",
      "Best Val Acc: 0.400000\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop_layer = nn.Dropout(p=0.1)\n",
    "        self.fc1 = nn.Linear(6, 128)\n",
    "        self.fc2 = nn.Linear(128, 96)\n",
    "        self.fc3 = nn.Linear(96, 64)\n",
    "        self.fc4 = nn.Linear(64, 48)\n",
    "        self.fc5 = nn.Linear(48, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "n_epochs = 30\n",
    "minibatch_size = 16\n",
    "learning_rate = 0.001\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "ce_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, X_train.shape[0], eta_min=learning_rate)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "mp_model = train_model(True, X1_train, y1_train, X1_val, y1_val, model, ce_criterion, optimizer, scheduler, minibatch_size, \"mp_classifier\", num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for MP Classification\n",
      "Test data, Loss: 1.37, Accuracy: 0.342\n"
     ]
    }
   ],
   "source": [
    "print \"Test Results for MP Classification\"\n",
    "predict(X1_test, y1_test, mp_model, ce_criterion, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Regressor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n",
      "Training Loss: 1.9939\n",
      "Validation Loss: 1.8672\n",
      "\n",
      "Epoch 1/29\n",
      "----------\n",
      "Training Loss: 1.8374\n",
      "Validation Loss: 1.8779\n",
      "\n",
      "Epoch 2/29\n",
      "----------\n",
      "Training Loss: 1.8271\n",
      "Validation Loss: 1.8778\n",
      "\n",
      "Epoch 3/29\n",
      "----------\n",
      "Training Loss: 1.8154\n",
      "Validation Loss: 1.8447\n",
      "\n",
      "Epoch 4/29\n",
      "----------\n",
      "Training Loss: 1.8089\n",
      "Validation Loss: 1.8614\n",
      "\n",
      "Epoch 5/29\n",
      "----------\n",
      "Training Loss: 1.8051\n",
      "Validation Loss: 1.8508\n",
      "\n",
      "Epoch 6/29\n",
      "----------\n",
      "Training Loss: 1.7996\n",
      "Validation Loss: 1.8460\n",
      "\n",
      "Epoch 7/29\n",
      "----------\n",
      "Training Loss: 1.7981\n",
      "Validation Loss: 1.8576\n",
      "\n",
      "Epoch 8/29\n",
      "----------\n",
      "Training Loss: 1.7949\n",
      "Validation Loss: 1.8463\n",
      "\n",
      "Epoch 9/29\n",
      "----------\n",
      "Training Loss: 1.7938\n",
      "Validation Loss: 1.8482\n",
      "\n",
      "Epoch 10/29\n",
      "----------\n",
      "Training Loss: 1.7881\n",
      "Validation Loss: 1.8472\n",
      "\n",
      "Epoch 11/29\n",
      "----------\n",
      "Training Loss: 1.7886\n",
      "Validation Loss: 1.8495\n",
      "\n",
      "Epoch 12/29\n",
      "----------\n",
      "Training Loss: 1.7868\n",
      "Validation Loss: 1.8469\n",
      "\n",
      "Epoch 13/29\n",
      "----------\n",
      "Training Loss: 1.7889\n",
      "Validation Loss: 1.8425\n",
      "\n",
      "Epoch 14/29\n",
      "----------\n",
      "Training Loss: 1.7842\n",
      "Validation Loss: 1.8446\n",
      "\n",
      "Epoch 15/29\n",
      "----------\n",
      "Training Loss: 1.7811\n",
      "Validation Loss: 1.8423\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n",
      "Training Loss: 1.7831\n",
      "Validation Loss: 1.8425\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n",
      "Training Loss: 1.7809\n",
      "Validation Loss: 1.8525\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n",
      "Training Loss: 1.7822\n",
      "Validation Loss: 1.8434\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n",
      "Training Loss: 1.7791\n",
      "Validation Loss: 1.8442\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n",
      "Training Loss: 1.7805\n",
      "Validation Loss: 1.8452\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n",
      "Training Loss: 1.7793\n",
      "Validation Loss: 1.8418\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n",
      "Training Loss: 1.7810\n",
      "Validation Loss: 1.8440\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n",
      "Training Loss: 1.7791\n",
      "Validation Loss: 1.8431\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n",
      "Training Loss: 1.7796\n",
      "Validation Loss: 1.8448\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n",
      "Training Loss: 1.7778\n",
      "Validation Loss: 1.8430\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n",
      "Training Loss: 1.7775\n",
      "Validation Loss: 1.8424\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n",
      "Training Loss: 1.7761\n",
      "Validation Loss: 1.8435\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n",
      "Training Loss: 1.7765\n",
      "Validation Loss: 1.8428\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n",
      "Training Loss: 1.7766\n",
      "Validation Loss: 1.8411\n",
      "\n",
      "Training complete in 21s\n",
      "Best Train Loss: 1.776588\n",
      "Best Val Loss: 1.841146\n"
     ]
    }
   ],
   "source": [
    "class Net_Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_Regressor, self).__init__()\n",
    "        self.drop_layer = nn.Dropout(p=0.15)\n",
    "        self.fc1 = nn.Linear(11, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "n_epochs = 30\n",
    "minibatch_size = 16\n",
    "learning_rate = 0.001\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = Net_Regressor()\n",
    "model = model.to(device)\n",
    "mse_criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, X_train.shape[0], eta_min=learning_rate)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "time_model = train_model(False, X2_train, y2_train, X2_val, y2_val, model, mse_criterion, optimizer, scheduler, minibatch_size, 'time_regressor', num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for Time Regression\n",
      "Test data, Loss: 0.651\n",
      "\n",
      "Actual - Predicted\n",
      "1.14 - 1.52\n",
      "1.93 - 1.56\n",
      "2.17 - 1.55\n",
      "0.868 - 1.52\n",
      "1.08 - 1.53\n",
      "1.68 - 1.52\n",
      "1.69 - 1.53\n",
      "1.19 - 1.54\n",
      "0.902 - 1.52\n",
      "1.0 - 1.52\n",
      "2.21 - 1.55\n",
      "0.921 - 1.53\n",
      "1.36 - 1.52\n",
      "0.894 - 1.51\n",
      "1.01 - 1.52\n",
      "1.91 - 1.54\n",
      "1.2 - 1.53\n",
      "1.44 - 1.52\n",
      "1.4 - 1.53\n",
      "1.5 - 1.55\n",
      "1.56 - 1.55\n",
      "1.98 - 1.53\n",
      "1.27 - 1.52\n",
      "1.24 - 1.55\n",
      "1.31 - 1.53\n",
      "1.68 - 1.54\n",
      "2.47 - 1.56\n",
      "1.36 - 1.54\n",
      "1.55 - 1.53\n",
      "1.77 - 1.53\n",
      "1.17 - 1.54\n",
      "2.67 - 1.58\n",
      "1.97 - 1.55\n",
      "1.46 - 1.54\n",
      "0.401 - 1.51\n",
      "2.14 - 1.6\n",
      "1.14 - 1.51\n",
      "1.55 - 1.54\n",
      "0.784 - 1.51\n",
      "1.27 - 1.54\n",
      "1.7 - 1.57\n",
      "1.49 - 1.56\n",
      "1.67 - 1.53\n",
      "1.57 - 1.55\n",
      "1.26 - 1.53\n",
      "1.18 - 1.52\n",
      "0.641 - 1.52\n",
      "1.36 - 1.54\n",
      "0.639 - 1.52\n",
      "1.58 - 1.54\n",
      "0.987 - 1.53\n",
      "2.44 - 1.56\n",
      "2.39 - 1.57\n",
      "1.23 - 1.54\n",
      "0.996 - 1.52\n",
      "0.982 - 1.53\n",
      "2.13 - 1.56\n",
      "1.29 - 1.53\n",
      "1.68 - 1.54\n",
      "1.05 - 1.52\n",
      "2.54 - 1.57\n",
      "1.04 - 1.53\n",
      "1.15 - 1.54\n",
      "2.11 - 1.54\n",
      "1.4 - 1.55\n",
      "1.3 - 1.53\n",
      "0.969 - 1.56\n",
      "0.87 - 1.52\n",
      "2.06 - 1.54\n",
      "1.1 - 1.53\n",
      "2.83 - 1.58\n",
      "1.31 - 1.54\n",
      "1.04 - 1.53\n",
      "0.987 - 1.53\n",
      "1.19 - 1.52\n",
      "1.02 - 1.53\n",
      "0.705 - 1.51\n",
      "1.33 - 1.53\n",
      "1.56 - 1.54\n",
      "1.27 - 1.54\n",
      "1.7 - 1.54\n",
      "2.02 - 1.54\n",
      "2.26 - 1.55\n",
      "2.06 - 1.54\n",
      "1.87 - 1.54\n",
      "1.27 - 1.52\n",
      "1.59 - 1.52\n",
      "1.36 - 1.52\n",
      "1.22 - 1.52\n",
      "1.24 - 1.52\n",
      "1.06 - 1.52\n",
      "2.74 - 1.61\n",
      "1.73 - 1.54\n",
      "1.35 - 1.54\n",
      "1.87 - 1.57\n",
      "1.38 - 1.54\n",
      "1.04 - 1.52\n",
      "1.67 - 1.52\n",
      "1.68 - 1.56\n",
      "1.5 - 1.53\n",
      "1.71 - 1.53\n",
      "1.02 - 1.52\n",
      "1.34 - 1.53\n",
      "1.88 - 1.54\n",
      "1.62 - 1.53\n",
      "7.38 - 1.74\n",
      "2.17 - 1.54\n",
      "0.987 - 1.51\n",
      "0.534 - 1.52\n",
      "1.42 - 1.54\n",
      "1.04 - 1.52\n",
      "1.92 - 1.54\n",
      "0.753 - 1.51\n",
      "0.527 - 1.52\n",
      "0.758 - 1.51\n",
      "1.6 - 1.54\n",
      "2.17 - 1.57\n",
      "1.24 - 1.52\n",
      "2.29 - 1.55\n",
      "1.05 - 1.52\n",
      "1.25 - 1.55\n",
      "2.37 - 1.6\n",
      "0.485 - 1.51\n",
      "0.836 - 1.53\n",
      "1.14 - 1.53\n",
      "0.679 - 1.53\n",
      "1.46 - 1.52\n",
      "0.915 - 1.52\n",
      "2.32 - 1.59\n",
      "0.59 - 1.5\n",
      "1.93 - 1.53\n",
      "2.13 - 1.55\n",
      "2.69 - 1.6\n",
      "2.11 - 1.56\n",
      "2.51 - 1.56\n",
      "1.55 - 1.53\n",
      "1.87 - 1.54\n",
      "2.22 - 1.56\n",
      "1.37 - 1.53\n",
      "1.09 - 1.53\n",
      "1.95 - 1.54\n",
      "2.45 - 1.56\n",
      "1.24 - 1.53\n",
      "1.18 - 1.53\n",
      "1.23 - 1.52\n",
      "0.682 - 1.51\n",
      "1.1 - 1.52\n",
      "1.56 - 1.52\n",
      "2.52 - 1.59\n",
      "2.46 - 1.57\n",
      "1.42 - 1.54\n",
      "1.45 - 1.52\n",
      "0.586 - 1.5\n",
      "2.16 - 1.54\n",
      "1.28 - 1.54\n",
      "0.955 - 1.52\n",
      "1.7 - 1.53\n",
      "2.03 - 1.55\n",
      "0.814 - 1.53\n",
      "2.02 - 1.56\n",
      "1.86 - 1.53\n",
      "1.32 - 1.52\n",
      "1.49 - 1.54\n",
      "0.699 - 1.52\n",
      "1.42 - 1.52\n",
      "2.76 - 1.58\n",
      "1.04 - 1.52\n",
      "2.39 - 1.56\n",
      "1.81 - 1.53\n",
      "1.25 - 1.52\n",
      "1.78 - 1.54\n",
      "2.59 - 1.55\n",
      "1.4 - 1.53\n",
      "1.22 - 1.55\n",
      "1.17 - 1.54\n",
      "1.18 - 1.53\n",
      "1.98 - 1.54\n",
      "1.09 - 1.52\n",
      "1.24 - 1.53\n",
      "0.958 - 1.52\n",
      "1.32 - 1.55\n",
      "0.807 - 1.52\n",
      "1.61 - 1.54\n",
      "1.87 - 1.57\n",
      "1.46 - 1.52\n",
      "1.8 - 1.55\n",
      "1.1 - 1.54\n",
      "1.85 - 1.55\n",
      "0.948 - 1.52\n",
      "7.01 - 1.81\n",
      "1.48 - 1.54\n",
      "1.55 - 1.55\n",
      "1.14 - 1.54\n",
      "0.683 - 1.51\n",
      "0.919 - 1.52\n",
      "1.63 - 1.53\n",
      "1.99 - 1.54\n",
      "1.6 - 1.55\n",
      "1.09 - 1.52\n",
      "1.93 - 1.54\n",
      "1.19 - 1.54\n",
      "1.31 - 1.54\n",
      "0.987 - 1.53\n",
      "1.58 - 1.54\n",
      "1.77 - 1.56\n",
      "1.57 - 1.55\n",
      "1.71 - 1.54\n",
      "0.702 - 1.51\n",
      "1.97 - 1.56\n",
      "1.51 - 1.57\n",
      "0.771 - 1.52\n",
      "1.26 - 1.52\n",
      "0.703 - 1.51\n",
      "1.69 - 1.54\n",
      "0.579 - 1.51\n",
      "1.05 - 1.53\n",
      "1.01 - 1.52\n",
      "0.737 - 1.51\n",
      "1.59 - 1.55\n",
      "1.14 - 1.54\n",
      "0.586 - 1.52\n",
      "0.994 - 1.51\n",
      "1.78 - 1.54\n",
      "1.41 - 1.55\n",
      "1.52 - 1.55\n",
      "2.59 - 1.6\n",
      "1.58 - 1.54\n",
      "1.32 - 1.53\n",
      "1.68 - 1.53\n",
      "0.585 - 1.5\n",
      "2.27 - 1.58\n",
      "1.67 - 1.55\n",
      "1.38 - 1.54\n",
      "2.07 - 1.54\n",
      "1.32 - 1.52\n",
      "1.96 - 1.54\n",
      "1.02 - 1.5\n",
      "1.18 - 1.53\n",
      "1.88 - 1.55\n",
      "1.59 - 1.56\n",
      "1.26 - 1.52\n",
      "1.99 - 1.55\n",
      "0.824 - 1.51\n",
      "0.907 - 1.51\n",
      "1.61 - 1.55\n",
      "2.74 - 1.61\n",
      "5.59 - 1.69\n",
      "1.02 - 1.53\n",
      "1.37 - 1.53\n",
      "1.17 - 1.53\n",
      "1.32 - 1.55\n",
      "0.935 - 1.52\n",
      "1.74 - 1.55\n",
      "2.22 - 1.58\n",
      "1.18 - 1.52\n",
      "1.46 - 1.53\n",
      "1.75 - 1.54\n",
      "0.783 - 1.52\n",
      "0.905 - 1.51\n",
      "1.83 - 1.58\n",
      "1.32 - 1.54\n",
      "1.3 - 1.54\n",
      "1.46 - 1.55\n",
      "2.34 - 1.56\n",
      "0.707 - 1.52\n",
      "1.31 - 1.52\n",
      "1.34 - 1.55\n",
      "2.25 - 1.57\n",
      "1.74 - 1.52\n",
      "1.29 - 1.52\n",
      "1.97 - 1.55\n",
      "1.19 - 1.53\n",
      "1.02 - 1.52\n",
      "1.25 - 1.52\n",
      "1.49 - 1.52\n",
      "1.56 - 1.55\n",
      "2.24 - 1.55\n",
      "1.77 - 1.54\n",
      "1.85 - 1.56\n",
      "0.965 - 1.51\n",
      "1.84 - 1.53\n",
      "1.03 - 1.51\n",
      "1.62 - 1.53\n",
      "1.24 - 1.52\n",
      "0.75 - 1.52\n",
      "1.72 - 1.57\n",
      "1.36 - 1.54\n",
      "1.41 - 1.51\n",
      "0.979 - 1.53\n",
      "1.59 - 1.54\n",
      "1.74 - 1.53\n",
      "1.13 - 1.54\n",
      "1.23 - 1.52\n",
      "1.18 - 1.53\n",
      "1.36 - 1.52\n",
      "1.18 - 1.52\n",
      "1.15 - 1.52\n",
      "1.79 - 1.54\n",
      "1.01 - 1.52\n",
      "1.7 - 1.53\n",
      "1.77 - 1.53\n",
      "1.3 - 1.54\n",
      "0.493 - 1.5\n",
      "2.25 - 1.55\n",
      "1.48 - 1.53\n",
      "1.05 - 1.52\n",
      "1.41 - 1.52\n",
      "1.53 - 1.55\n",
      "2.96 - 1.66\n",
      "1.72 - 1.54\n",
      "0.96 - 1.52\n",
      "2.03 - 1.55\n",
      "1.82 - 1.55\n",
      "1.36 - 1.54\n",
      "0.65 - 1.48\n",
      "1.71 - 1.54\n",
      "1.56 - 1.55\n",
      "1.59 - 1.55\n",
      "1.45 - 1.52\n",
      "1.52 - 1.53\n",
      "2.33 - 1.56\n",
      "1.16 - 1.53\n",
      "1.02 - 1.52\n",
      "1.79 - 1.57\n",
      "1.27 - 1.53\n",
      "1.4 - 1.54\n",
      "0.917 - 1.53\n",
      "2.02 - 1.55\n",
      "1.46 - 1.55\n",
      "1.0 - 1.53\n",
      "1.06 - 1.52\n",
      "1.61 - 1.57\n",
      "2.44 - 1.58\n",
      "1.69 - 1.57\n",
      "0.802 - 1.52\n",
      "1.92 - 1.53\n",
      "0.594 - 1.51\n",
      "1.07 - 1.52\n",
      "1.13 - 1.52\n",
      "0.863 - 1.52\n",
      "1.93 - 1.54\n",
      "1.01 - 1.53\n",
      "1.04 - 1.54\n",
      "1.56 - 1.54\n",
      "2.05 - 1.58\n",
      "2.15 - 1.57\n",
      "0.64 - 1.52\n",
      "1.1 - 1.53\n",
      "1.19 - 1.52\n",
      "1.96 - 1.54\n",
      "1.6 - 1.54\n",
      "2.09 - 1.55\n",
      "2.27 - 1.56\n",
      "0.754 - 1.52\n",
      "1.4 - 1.55\n",
      "1.39 - 1.54\n",
      "1.78 - 1.56\n",
      "1.42 - 1.53\n",
      "2.4 - 1.58\n",
      "0.799 - 1.51\n",
      "1.59 - 1.55\n",
      "1.14 - 1.54\n",
      "2.26 - 1.54\n",
      "1.84 - 1.56\n",
      "2.18 - 1.56\n",
      "1.36 - 1.54\n",
      "7.83 - 1.76\n",
      "0.929 - 1.51\n",
      "1.77 - 1.54\n",
      "1.96 - 1.57\n",
      "1.94 - 1.55\n",
      "1.64 - 1.54\n",
      "1.22 - 1.52\n",
      "1.05 - 1.52\n",
      "0.921 - 1.5\n",
      "1.83 - 1.56\n",
      "1.72 - 1.55\n",
      "0.864 - 1.51\n",
      "1.49 - 1.53\n",
      "1.31 - 1.52\n",
      "0.489 - 1.51\n",
      "1.51 - 1.55\n",
      "0.552 - 1.5\n",
      "1.23 - 1.53\n",
      "1.67 - 1.55\n",
      "1.47 - 1.54\n",
      "2.04 - 1.52\n",
      "1.79 - 1.54\n",
      "1.49 - 1.53\n",
      "1.36 - 1.53\n",
      "1.46 - 1.52\n",
      "0.799 - 1.51\n",
      "1.76 - 1.54\n",
      "0.858 - 1.52\n",
      "6.43 - 1.8\n",
      "3.17 - 1.57\n",
      "1.84 - 1.57\n",
      "1.31 - 1.53\n",
      "1.68 - 1.53\n",
      "1.62 - 1.56\n",
      "1.36 - 1.53\n",
      "1.36 - 1.54\n",
      "1.25 - 1.54\n",
      "1.83 - 1.57\n",
      "0.995 - 1.53\n",
      "2.04 - 1.58\n",
      "0.844 - 1.51\n",
      "1.95 - 1.55\n",
      "2.31 - 1.6\n",
      "1.53 - 1.53\n",
      "2.02 - 1.55\n",
      "1.31 - 1.54\n",
      "0.809 - 1.51\n",
      "0.766 - 1.53\n",
      "1.39 - 1.53\n",
      "1.47 - 1.53\n",
      "0.555 - 1.52\n",
      "0.619 - 1.51\n",
      "0.78 - 1.51\n",
      "1.06 - 1.53\n",
      "2.97 - 1.6\n",
      "0.869 - 1.52\n",
      "1.58 - 1.52\n",
      "1.45 - 1.53\n",
      "1.15 - 1.54\n",
      "1.5 - 1.54\n",
      "1.99 - 1.54\n",
      "1.65 - 1.55\n",
      "0.393 - 1.51\n",
      "0.858 - 1.51\n",
      "1.43 - 1.53\n",
      "0.544 - 1.5\n",
      "1.21 - 1.52\n",
      "1.15 - 1.53\n",
      "1.77 - 1.54\n",
      "1.6 - 1.55\n",
      "1.63 - 1.52\n",
      "0.859 - 1.51\n",
      "1.81 - 1.55\n",
      "1.86 - 1.54\n",
      "1.71 - 1.55\n",
      "1.2 - 1.53\n",
      "1.81 - 1.53\n",
      "1.7 - 1.53\n",
      "0.688 - 1.51\n",
      "1.18 - 1.53\n",
      "1.94 - 1.58\n",
      "1.87 - 1.55\n",
      "1.74 - 1.56\n",
      "0.663 - 1.51\n",
      "0.44 - 1.51\n",
      "0.968 - 1.53\n",
      "2.64 - 1.63\n",
      "1.91 - 1.55\n",
      "1.65 - 1.55\n",
      "1.17 - 1.53\n",
      "1.35 - 1.52\n",
      "1.6 - 1.53\n",
      "1.8 - 1.54\n",
      "0.924 - 1.51\n",
      "1.8 - 1.55\n",
      "2.7 - 1.57\n",
      "1.19 - 1.53\n",
      "1.11 - 1.52\n",
      "2.32 - 1.58\n",
      "1.24 - 1.51\n",
      "1.56 - 1.54\n",
      "1.21 - 1.53\n",
      "7.23 - 1.74\n",
      "2.3 - 1.57\n",
      "1.55 - 1.53\n",
      "1.33 - 1.53\n",
      "3.91 - 1.73\n",
      "0.856 - 1.51\n",
      "1.91 - 1.54\n",
      "1.09 - 1.54\n",
      "2.22 - 1.57\n",
      "1.63 - 1.52\n",
      "1.31 - 1.54\n",
      "1.14 - 1.54\n",
      "2.37 - 1.6\n",
      "2.18 - 1.56\n",
      "2.23 - 1.56\n",
      "1.83 - 1.56\n",
      "1.16 - 1.54\n",
      "1.93 - 1.56\n",
      "1.85 - 1.54\n",
      "1.87 - 1.56\n",
      "2.22 - 1.54\n",
      "2.02 - 1.58\n",
      "1.5 - 1.52\n",
      "1.13 - 1.53\n",
      "3.66 - 1.59\n",
      "1.86 - 1.55\n",
      "1.39 - 1.53\n",
      "1.23 - 1.52\n",
      "1.67 - 1.53\n",
      "1.4 - 1.55\n",
      "1.4 - 1.55\n",
      "1.32 - 1.54\n",
      "1.48 - 1.53\n",
      "2.19 - 1.55\n",
      "0.34 - 1.51\n",
      "1.72 - 1.53\n",
      "0.853 - 1.51\n",
      "1.78 - 1.54\n",
      "1.34 - 1.53\n",
      "1.35 - 1.54\n",
      "1.29 - 1.52\n",
      "0.9 - 1.53\n",
      "1.42 - 1.54\n",
      "1.4 - 1.52\n",
      "1.48 - 1.52\n",
      "0.321 - 1.48\n",
      "1.17 - 1.52\n",
      "2.19 - 1.55\n",
      "1.15 - 1.53\n",
      "1.99 - 1.55\n",
      "1.32 - 1.54\n",
      "1.39 - 1.52\n",
      "1.72 - 1.53\n",
      "1.39 - 1.54\n",
      "1.2 - 1.54\n",
      "0.815 - 1.52\n",
      "1.43 - 1.53\n",
      "1.78 - 1.54\n",
      "2.8 - 1.62\n",
      "1.92 - 1.54\n",
      "0.931 - 1.52\n",
      "1.08 - 1.54\n",
      "1.9 - 1.54\n",
      "2.19 - 1.55\n",
      "1.59 - 1.55\n",
      "2.08 - 1.56\n",
      "2.12 - 1.54\n",
      "1.49 - 1.53\n",
      "1.27 - 1.53\n",
      "0.728 - 1.52\n",
      "1.38 - 1.53\n",
      "0.977 - 1.52\n",
      "1.56 - 1.54\n",
      "0.958 - 1.52\n",
      "1.11 - 1.54\n",
      "1.64 - 1.55\n",
      "2.29 - 1.57\n",
      "1.47 - 1.54\n",
      "0.781 - 1.52\n",
      "1.73 - 1.53\n",
      "1.96 - 1.56\n",
      "3.92 - 1.59\n",
      "1.75 - 1.53\n",
      "1.59 - 1.57\n",
      "1.23 - 1.52\n",
      "1.89 - 1.55\n",
      "0.994 - 1.53\n",
      "2.02 - 1.56\n",
      "1.29 - 1.53\n",
      "1.08 - 1.54\n",
      "1.53 - 1.53\n",
      "1.57 - 1.54\n",
      "1.7 - 1.54\n",
      "1.91 - 1.54\n",
      "1.38 - 1.55\n",
      "2.14 - 1.55\n",
      "1.1 - 1.53\n",
      "0.986 - 1.52\n",
      "1.99 - 1.55\n",
      "1.31 - 1.52\n",
      "0.816 - 1.49\n",
      "2.75 - 1.58\n",
      "1.22 - 1.52\n",
      "1.18 - 1.55\n",
      "1.7 - 1.55\n",
      "1.16 - 1.54\n",
      "0.958 - 1.53\n",
      "1.3 - 1.52\n",
      "1.25 - 1.53\n",
      "1.61 - 1.54\n",
      "2.45 - 1.56\n",
      "1.66 - 1.57\n",
      "1.68 - 1.57\n",
      "0.921 - 1.51\n",
      "1.52 - 1.55\n",
      "1.18 - 1.53\n",
      "1.74 - 1.56\n",
      "2.23 - 1.56\n",
      "0.992 - 1.53\n",
      "2.35 - 1.58\n",
      "1.54 - 1.53\n",
      "1.18 - 1.49\n",
      "1.72 - 1.55\n",
      "0.859 - 1.52\n",
      "0.848 - 1.52\n",
      "2.74 - 1.59\n",
      "1.13 - 1.51\n",
      "1.01 - 1.53\n",
      "1.4 - 1.53\n",
      "1.65 - 1.56\n",
      "1.3 - 1.53\n",
      "1.42 - 1.52\n",
      "0.967 - 1.53\n",
      "1.38 - 1.54\n",
      "1.17 - 1.52\n",
      "2.15 - 1.54\n",
      "1.2 - 1.52\n",
      "1.74 - 1.53\n",
      "1.36 - 1.54\n",
      "1.56 - 1.54\n",
      "1.41 - 1.52\n",
      "0.798 - 1.49\n",
      "2.2 - 1.56\n",
      "2.02 - 1.59\n",
      "1.38 - 1.54\n",
      "1.91 - 1.53\n",
      "1.12 - 1.51\n",
      "1.36 - 1.53\n",
      "0.885 - 1.52\n",
      "1.96 - 1.55\n",
      "1.7 - 1.55\n",
      "1.26 - 1.52\n",
      "2.54 - 1.57\n",
      "2.52 - 1.6\n",
      "1.38 - 1.53\n",
      "0.584 - 1.51\n",
      "0.995 - 1.52\n",
      "1.85 - 1.54\n",
      "1.41 - 1.52\n",
      "1.4 - 1.53\n",
      "0.441 - 1.51\n",
      "2.06 - 1.56\n",
      "1.68 - 1.55\n",
      "1.23 - 1.53\n",
      "1.97 - 1.53\n",
      "0.741 - 1.52\n",
      "1.51 - 1.55\n",
      "1.87 - 1.53\n",
      "2.25 - 1.55\n",
      "1.69 - 1.53\n",
      "0.59 - 1.51\n",
      "1.33 - 1.53\n",
      "1.52 - 1.55\n",
      "0.992 - 1.52\n",
      "2.35 - 1.55\n",
      "0.854 - 1.53\n",
      "1.65 - 1.55\n",
      "0.138 - 1.51\n",
      "1.04 - 1.52\n",
      "0.973 - 1.53\n",
      "1.69 - 1.53\n",
      "1.54 - 1.54\n",
      "1.6 - 1.57\n",
      "1.74 - 1.52\n",
      "1.79 - 1.54\n",
      "1.14 - 1.53\n",
      "1.4 - 1.57\n",
      "1.16 - 1.53\n",
      "1.05 - 1.52\n",
      "1.66 - 1.53\n",
      "1.28 - 1.53\n",
      "1.13 - 1.52\n",
      "1.59 - 1.52\n",
      "1.17 - 1.52\n",
      "1.72 - 1.53\n",
      "7.0 - 1.8\n",
      "1.59 - 1.53\n",
      "3.54 - 1.59\n",
      "2.67 - 1.6\n",
      "1.57 - 1.53\n",
      "1.69 - 1.53\n",
      "1.43 - 1.52\n",
      "1.54 - 1.52\n",
      "1.2 - 1.54\n",
      "0.822 - 1.5\n",
      "1.51 - 1.54\n",
      "2.38 - 1.58\n",
      "1.19 - 1.55\n",
      "2.37 - 1.58\n",
      "2.0 - 1.54\n",
      "0.815 - 1.52\n",
      "1.55 - 1.52\n",
      "1.31 - 1.53\n",
      "1.18 - 1.53\n",
      "1.65 - 1.53\n",
      "6.38 - 1.69\n",
      "1.95 - 1.55\n",
      "2.45 - 1.57\n",
      "0.97 - 1.52\n",
      "1.92 - 1.55\n",
      "1.52 - 1.54\n",
      "1.9 - 1.54\n",
      "1.67 - 1.53\n",
      "1.86 - 1.54\n",
      "1.8 - 1.53\n",
      "1.77 - 1.57\n",
      "1.55 - 1.54\n",
      "1.13 - 1.52\n",
      "1.8 - 1.54\n",
      "1.44 - 1.56\n",
      "0.997 - 1.52\n",
      "1.36 - 1.51\n",
      "1.6 - 1.52\n",
      "0.876 - 1.51\n",
      "1.52 - 1.52\n",
      "0.902 - 1.51\n",
      "1.07 - 1.53\n",
      "1.66 - 1.55\n",
      "1.76 - 1.55\n",
      "1.35 - 1.52\n",
      "0.842 - 1.51\n",
      "1.77 - 1.57\n",
      "1.46 - 1.53\n",
      "0.856 - 1.52\n",
      "1.26 - 1.52\n",
      "2.57 - 1.57\n",
      "1.84 - 1.55\n",
      "1.47 - 1.54\n",
      "1.09 - 1.56\n",
      "1.28 - 1.52\n",
      "2.18 - 1.55\n",
      "0.991 - 1.51\n",
      "1.3 - 1.53\n",
      "1.36 - 1.54\n",
      "0.897 - 1.52\n",
      "1.48 - 1.53\n",
      "1.98 - 1.54\n",
      "0.128 - 1.5\n",
      "1.82 - 1.55\n",
      "1.25 - 1.53\n",
      "2.0 - 1.54\n",
      "1.33 - 1.53\n",
      "1.1 - 1.53\n",
      "1.28 - 1.54\n",
      "1.42 - 1.53\n",
      "1.1 - 1.51\n",
      "2.03 - 1.54\n",
      "1.71 - 1.52\n",
      "1.39 - 1.52\n",
      "6.43 - 1.8\n",
      "0.895 - 1.53\n",
      "1.86 - 1.58\n",
      "1.21 - 1.52\n",
      "1.05 - 1.52\n",
      "2.21 - 1.55\n",
      "1.48 - 1.51\n",
      "2.24 - 1.55\n",
      "1.72 - 1.54\n",
      "1.39 - 1.52\n",
      "1.54 - 1.55\n",
      "1.28 - 1.52\n",
      "2.53 - 1.59\n",
      "1.9 - 1.53\n",
      "1.74 - 1.54\n",
      "2.55 - 1.6\n",
      "0.972 - 1.52\n",
      "2.18 - 1.56\n",
      "1.14 - 1.53\n",
      "1.37 - 1.53\n",
      "1.43 - 1.52\n",
      "2.04 - 1.55\n",
      "2.49 - 1.56\n",
      "1.41 - 1.52\n",
      "0.714 - 1.52\n",
      "1.7 - 1.55\n",
      "0.585 - 1.51\n",
      "1.43 - 1.53\n",
      "2.05 - 1.56\n",
      "0.798 - 1.51\n",
      "2.06 - 1.56\n",
      "0.502 - 1.5\n",
      "1.71 - 1.54\n",
      "2.34 - 1.58\n",
      "1.37 - 1.53\n",
      "0.923 - 1.52\n",
      "0.735 - 1.51\n",
      "1.46 - 1.53\n",
      "2.2 - 1.57\n",
      "1.05 - 1.52\n",
      "0.953 - 1.52\n",
      "0.582 - 1.51\n",
      "0.858 - 1.53\n",
      "1.54 - 1.52\n",
      "1.38 - 1.53\n",
      "0.754 - 1.52\n",
      "1.26 - 1.55\n",
      "2.18 - 1.55\n",
      "1.65 - 1.53\n",
      "7.17 - 1.8\n",
      "1.86 - 1.55\n",
      "1.69 - 1.58\n",
      "1.28 - 1.54\n",
      "1.18 - 1.53\n",
      "0.941 - 1.52\n",
      "0.915 - 1.52\n",
      "2.09 - 1.54\n",
      "1.14 - 1.52\n",
      "2.21 - 1.57\n",
      "0.423 - 1.5\n",
      "1.19 - 1.53\n",
      "1.42 - 1.52\n",
      "1.29 - 1.54\n",
      "1.19 - 1.51\n",
      "1.06 - 1.52\n",
      "1.77 - 1.57\n",
      "1.44 - 1.53\n",
      "1.87 - 1.53\n",
      "1.52 - 1.53\n",
      "1.88 - 1.55\n",
      "1.96 - 1.55\n",
      "0.649 - 1.52\n",
      "1.74 - 1.55\n",
      "0.894 - 1.52\n",
      "2.35 - 1.57\n",
      "5.56 - 1.85\n",
      "1.11 - 1.52\n",
      "1.95 - 1.57\n",
      "1.5 - 1.54\n",
      "0.98 - 1.53\n",
      "1.09 - 1.52\n",
      "1.08 - 1.52\n",
      "0.985 - 1.52\n",
      "2.04 - 1.54\n",
      "2.41 - 1.58\n",
      "1.44 - 1.55\n",
      "2.21 - 1.56\n",
      "0.552 - 1.52\n",
      "1.86 - 1.54\n",
      "1.66 - 1.55\n",
      "3.31 - 1.62\n",
      "2.2 - 1.54\n",
      "2.21 - 1.58\n",
      "1.97 - 1.54\n",
      "2.87 - 1.58\n",
      "1.13 - 1.53\n",
      "2.13 - 1.56\n",
      "1.52 - 1.53\n",
      "2.49 - 1.55\n",
      "1.25 - 1.51\n",
      "1.45 - 1.53\n",
      "1.78 - 1.54\n",
      "1.16 - 1.53\n",
      "1.23 - 1.52\n",
      "2.59 - 1.58\n",
      "1.31 - 1.52\n",
      "3.59 - 1.61\n",
      "0.255 - 1.5\n",
      "0.939 - 1.52\n",
      "1.82 - 1.56\n",
      "0.909 - 1.52\n",
      "1.11 - 1.53\n",
      "0.51 - 1.51\n",
      "0.23 - 1.51\n",
      "0.961 - 1.52\n",
      "1.31 - 1.56\n",
      "1.38 - 1.54\n",
      "0.289 - 1.51\n",
      "1.39 - 1.54\n",
      "0.542 - 1.51\n",
      "1.75 - 1.55\n",
      "1.71 - 1.54\n",
      "2.62 - 1.57\n",
      "1.52 - 1.52\n",
      "1.3 - 1.54\n",
      "2.26 - 1.55\n",
      "1.98 - 1.55\n",
      "1.48 - 1.52\n",
      "1.38 - 1.53\n",
      "1.43 - 1.54\n",
      "1.3 - 1.53\n",
      "0.548 - 1.5\n",
      "1.59 - 1.53\n",
      "1.79 - 1.53\n",
      "1.79 - 1.55\n",
      "1.26 - 1.52\n",
      "0.773 - 1.52\n",
      "0.635 - 1.51\n",
      "0.806 - 1.51\n",
      "2.41 - 1.58\n",
      "1.81 - 1.54\n",
      "1.96 - 1.53\n",
      "0.979 - 1.52\n",
      "1.07 - 1.53\n",
      "1.8 - 1.53\n",
      "1.57 - 1.53\n",
      "1.45 - 1.54\n",
      "1.28 - 1.52\n",
      "1.44 - 1.52\n",
      "1.04 - 1.53\n",
      "3.24 - 1.69\n",
      "0.612 - 1.51\n",
      "1.07 - 1.51\n",
      "0.945 - 1.51\n",
      "0.524 - 1.5\n",
      "1.71 - 1.54\n",
      "1.68 - 1.53\n",
      "0.426 - 1.5\n",
      "1.93 - 1.54\n",
      "2.28 - 1.57\n",
      "1.49 - 1.55\n",
      "1.62 - 1.54\n",
      "1.97 - 1.55\n",
      "1.33 - 1.53\n",
      "1.41 - 1.52\n",
      "1.88 - 1.54\n",
      "1.58 - 1.54\n",
      "1.54 - 1.53\n",
      "0.874 - 1.52\n",
      "1.21 - 1.51\n",
      "2.3 - 1.56\n",
      "2.34 - 1.58\n",
      "0.616 - 1.52\n",
      "1.1 - 1.52\n",
      "1.44 - 1.55\n",
      "1.71 - 1.53\n",
      "1.05 - 1.52\n",
      "1.93 - 1.55\n",
      "0.866 - 1.52\n",
      "1.05 - 1.53\n",
      "1.47 - 1.54\n",
      "1.76 - 1.56\n",
      "1.87 - 1.56\n",
      "2.01 - 1.54\n",
      "5.08 - 1.73\n",
      "1.79 - 1.53\n",
      "0.765 - 1.51\n",
      "1.3 - 1.52\n",
      "0.856 - 1.52\n",
      "1.3 - 1.54\n",
      "1.47 - 1.54\n",
      "1.71 - 1.54\n",
      "1.16 - 1.53\n",
      "1.27 - 1.53\n",
      "2.28 - 1.57\n",
      "1.26 - 1.53\n",
      "1.26 - 1.52\n",
      "1.49 - 1.53\n",
      "0.9 - 1.56\n",
      "1.16 - 1.54\n",
      "2.06 - 1.55\n",
      "1.17 - 1.52\n",
      "1.96 - 1.56\n",
      "0.636 - 1.51\n",
      "1.53 - 1.56\n",
      "2.16 - 1.58\n",
      "1.82 - 1.54\n",
      "1.21 - 1.52\n",
      "1.65 - 1.54\n",
      "2.02 - 1.55\n",
      "1.48 - 1.54\n",
      "1.18 - 1.54\n",
      "1.44 - 1.53\n",
      "1.26 - 1.52\n",
      "1.17 - 1.53\n",
      "2.09 - 1.56\n",
      "0.761 - 1.49\n",
      "1.22 - 1.52\n",
      "1.11 - 1.53\n",
      "0.957 - 1.51\n",
      "4.73 - 1.56\n",
      "1.49 - 1.54\n",
      "1.47 - 1.53\n",
      "7.01 - 1.8\n",
      "1.65 - 1.53\n",
      "0.926 - 1.52\n",
      "2.07 - 1.57\n",
      "1.38 - 1.54\n",
      "0.878 - 1.52\n",
      "1.52 - 1.54\n",
      "0.97 - 1.52\n",
      "0.911 - 1.53\n",
      "1.34 - 1.52\n",
      "1.56 - 1.52\n",
      "1.81 - 1.55\n",
      "2.88 - 1.57\n",
      "0.485 - 1.5\n",
      "0.651 - 1.51\n",
      "1.61 - 1.52\n",
      "0.618 - 1.52\n",
      "0.803 - 1.51\n",
      "1.17 - 1.53\n",
      "2.35 - 1.59\n",
      "1.06 - 1.52\n",
      "1.23 - 1.52\n",
      "0.664 - 1.52\n",
      "1.91 - 1.54\n",
      "0.764 - 1.52\n",
      "2.39 - 1.57\n",
      "0.753 - 1.51\n",
      "2.03 - 1.55\n",
      "1.26 - 1.52\n",
      "2.01 - 1.54\n",
      "2.38 - 1.55\n",
      "1.5 - 1.55\n",
      "2.8 - 1.6\n",
      "2.44 - 1.57\n",
      "1.68 - 1.54\n",
      "0.769 - 1.52\n",
      "0.687 - 1.53\n",
      "1.45 - 1.53\n",
      "0.229 - 1.5\n",
      "1.55 - 1.55\n"
     ]
    }
   ],
   "source": [
    "print \"Test Results for Time Regression\"\n",
    "predict(X2_test, y2_test, time_model, mse_criterion, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
